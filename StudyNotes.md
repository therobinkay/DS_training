# DS_training

## 머신러닝
1. 머신러닝의 정의는 무엇인가요?

알고리즘을 통해 시스템을 발전시켜 세부적인 명령 없이도 자동적으로 습득하고, 패턴을 찾고, 결정을 내리는 기계를 만들어 내는 것.

2. 지도학습과 비지도학습의 차이가 무엇인지 상세한 예를 들어 설명하세요.

- 지도학습이란 label된 training data로부터 모델을 추측하는 것이다. Target label을 바탕으로 변수 간 상관관계를 파악하거나 예측 모델을 만든다. 종류로는 SVM, 인공지능망, 선형회귀, 로지스틱 회귀, XGBoost 등이 있다. 예) 집 평수에 근거하여 가격 예측, 검색 엔진 결과의 관계성 예측 등
- 비지도학습이란 label이 없는 data에서 숨겨진 상관관계나 패턴을 파악하는 것이다. Target label은 존재하지 않는다. 종류로는 clustering, PCA, SVD 등이 있다. 예) 투표에 의거해 유권자들 분류, 이미지 세분화 등

3. 머신러닝에서 overfitting일 경우 어떻게 대처해야 할까요?
- 모델을 단순화. 고려된 관련 피처의 개수를 줄임으로써 데이터의 노이즈를 줄인다.
- Cross-validation.
- L1, L2 등 정규화.


Cross Validation은 무엇이고 어떻게 해야하나요?
통계적 분석이 독립적인 데이터셋을 분석할 때 얼마나 일반화되는지 조사하는 model validation 방식. 보통 예측 모델의 성능을 조사할 때, overfitting을 방지하고 독립적인 데이터셋에 얼마나 잘 반응하는지 보기 위해 training phase에서 행해진다.
train set을 다시 한 번 train set + validation set으로 나누어 학습 중 검증과 수정을 하는 것을 의미. 적은 데이터로도 정확도를 향상시킬 수 있지만, iteration 횟수가 많아 훈련과 평가에 시간이 오래 걸리는 편.
종류로는 크게 k-fold, stratified k-fold, leave-one-out (LOOCV) 등이 있다.
K-fold: 가장 일반적. 데이터를 k개로 나누고 하나를 validation set으로 지정해, k번 iterate하며 train으로 학습하고 validation으로 테스트한 후 평균을 구한다.
Stratified k-fold: 주로 분류 문제에 사용. Label의 분포가 불균형할 때 분포를 고려해 validate.
LOOCV: 하나의 데이터를 제외하고 나머지를 모두 학습에 사용. K-fold보다 bias는 낮으나 variance가 높을 수 있음. 학습된 train set이 전체 데이터셋과 유사하고 평균의 분산값은 결과값의 correlation이 높을 수록 증가하기 때문.
(보통은 5-fold, 10-fold를 사용. Bias-variance trade-off에서 좋은 성적을 보이기에.)


Bias, variance, bias-variance tradeoff에 대해 각각 설명하고, overfitting과 underfitting이 이에 어떤 영향을 끼치는지 관련지어 설명하세요.
Bias(편향): 리턴 값들이 최적 값에서 얼마나 떨어진 곳에 집중되어 있는가?
Variance(분산): 리턴 값들이 얼마나 넓게 퍼져 있는가?
Bias가 높을수록 underfitting, variance가 높을수록 overfitting.
Bias와 variance는 trade-off 관계에 있으며, 이 둘 모두가 적절히 낮을 때 좋은 모델이라 할 수 있다.


일반적인 모델이 나을까요, 정확한 모델이 나을까요?
물론 task에 따라 다르다. 결국은 일반화가 어느 정도 가능하며, 동시에 데이터의 패턴을 찾아낼 수 있어야 한다. Bias-variance trade-off에서 좋은 균형을 찾을 수 있어야 한다. (좋은 균형을 찾아내려면 앙상블 모델이 주효할 때가 많다.)
복잡한 모델이 간결한 모델에 비해 유의미한 성능 향상이 없을 경우, 후자를 택한다.


Metric이란? Model accuracy를 측정할 때 가장 많이 사용하는 metric은?
모델의 성능 평가를 할 때 사용되는 지표다. 하려는 모델링이 회귀 문제인지, 분류 문제인지를 알아야 하며, label의 분포는 어떻게 되는지, task의 종류와 목표는 어떻게 되는지 등을 모두 고려해 주 metric을 결정해야 한다.


회귀와 분류에 대해 설명해주세요. 회귀 / 분류시 알맞은 metric은 무엇일까요?
회귀는 어떠한 값을 예측할 때 주로 사용하며, 분류는 데이터가 각각 어떠한 군(group)에 속하는지 판별할 때 주로 사용한다. 즉, 종속변수가 양적 데이터이면 회귀를, 범주형 데이터이면 분류를 사용한다.
회귀 시 주로 사용하는 metric은 다음과 같다: MAE, RMSE, 결정계수(R^2)
분류 시 주로 사용하는 metric은 다음과 같다: Accuracy, Recall, Precision, Specificity, F1 Score, ROC/AUC, log loss


알고 있는 metric에 대해 설명해주세요(ex. RMSE, MAE, recall, precision …)
MAE: error에 절대값을 적용해 평균. 직관적이나 단위에 의존적이고 error가 overestimate인지 underestimate인지 파악할 수 없음.
RMSE: error를 제곱하고 합한 후 제곱근. MAE와 비슷하나 이상치에 더 민감. 즉 큰 이상치에 더 많은 페널티를 준다.
결정계수(R^2): 예측 모델에 의해 설명되는 종속 변수의 비율. 분산을 기반으로 예측 성능을 평가. 1에 가까울 수록 좋으나, 우연한 상관성이 있거나 독립 변수가 많아질수록 증가해 단순 지표로만 사용하기에는 위험.
정확도(Accuracy): 정확히 예측한 값의 비율. ((TP+TN)/전체) 불균형한 분포에 취약해 좋지 않다.
Precision: 참으로 판별한 것들 중 실제 참의 비율. (TP/(TP+FP)) Recall과는 trade-off 관계.
Recall: 실제 참인 것들 중 참으로 판별한 비율. (TP/(TP+FN)) Precision과는 trade-off 관계.
F1-score: Precision과 Recall 모두 불균형한 분포에 민감하기 때문에, 그 둘의 역수를 취해 결합한 새로운 지표.
Specificity: 실제 거짓인 것들 중 거짓으로 판별한 비율. (TN/(TN+FP))
ROC curve: Recall(y) vs. 1-Specificity(x). 이진 분류의 성능을 판별할 때 쓰이며, 불균형한 분포에 민감하지 않다.
AUC: ROC curve 하의 면적. 1에 가까울수록 좋은 성능이다.
log loss: 확률적으로 잘못 분류를 했을 때 페널티를 증가시키는 방식. 확률 값을 지표로 사용하여 정답 확률이 낮아질수록 log loss 값은 기하급수적으로 증가.


왜 MSE는 좋지 않은 metric일까요? 대신 무엇을 쓰는 게 좋을까요?
큰 오차 (이상치)에 민감하고 단위가 다르기에 직관적이지도 않다. 주로 보다 직관적이며 장점은 계승한 RMSE를 사용한다.


Confusion Matrix는 무엇인가요?
분류 모델을 평가할 때 사용되는 지표인 Precision, Recall 등을 계산할 TP, TN, FP, FN을 표현한 행렬.


False positive와 false negative가 무엇인지, 왜 중요한지 설명하시고, 언제 각각의 error가 다른 error보다 중요한지 case를 들어 설명해주세요.
Confusion Matrix에 포함된 지표. FP는 실제로 음성이나 양성으로 잘못 판정된 수, FN은 실제로 양성이나 음성으로 잘못 판정된 수. 분류 모델 성능 지표에 사용되는 Precision, Recall, F1 score, Specificity, ROC/AUC에 사용되기에 매우 중요하다.
치명도가 낮으나 치료 과정이 고된 경우 (HIV), 또는 제한된 재정으로 광고나 물품을 보내는 경우, FP를 낮추는 것이 중요하다.
빠른 발견이 필요한 질병의 경우, 불량품을 판별하는 경우, 또는 물품의 출고가 그리 비싸지 않고 보다 많은 고객을 확보하는 데 중점을 둘 경우, FN을 낮추는 것이 중요하다.


Type 1 Error와 Type 2 Error는 무엇입니까?
Type 1 Error: H0을 reject해서는 안 되나, reject했을 때 (FP)
Type 2 Error: H0을 reject해야 하나, 그러지 못했을 때 (FN)


정규화(Regularization)를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?
정규화는 모델의 일반화를 위해 필요하며, 모델의 복잡도를 낮춘다. 따라서 Overfitting을 방지하는 데 효과적이다. (차수가 높아질수록, 즉 모델의 복잡도가 높아질수록 overfitting되며 variance가 올라간다. 즉, 정규화를 시킴으로서 복잡도를 낮추고 variance를 낮춘다.)
정규화는 회귀 계수 β값들에 제약을 줌으로써 모델의 복잡도를 조절하며 variance 값을 조정한다. (bias는 조금 생겨날지언정)
정규화의 방법에는 L1, L2, ElasticNet이 있다.


L1, L2 정규화에 대해 설명해주세요. 장단점은?
L2 (Ridge): 계수의 제곱을 활용한다. 회귀계수의 제곱의 합을 특정 값 λ와 곱한다. λ값이 낮을수록 β값은 높아지며, 종국에는 복잡한 MSE 식과 같아진다. (즉 overfitting된다.) λ값이 높아질수록 β값은 낮아지며, 모델은 간단해지고 variance가 줄어든다. (물론 너무 줄일 경우 underfitting되며 bias가 과도하게 높아진다.)
L1 (Lasso): 계수의 절대값을 활용한다. 회귀계수의 절대값의 합을 특정 값 λ와 곱한다. λ값의 변화가 β값에 미치는 영향은 L2 정규화와 비슷하다. L1 정규화는 β값에 영향을 주는 것 이외에도 예측에 불필요한 회귀계수들을 모두 0으로 처리하며, 유의미한 회귀계수들만 선택하는 역할도 한다. 꽤 robust한 모델이기에, 다른 training set을 고르더라도 계수 선택에는 큰 변화가 없다.
(t값 역시 λ값과 비슷하다. 회귀계수의 제곱의 합을 특정 값 t보다 적도록 제한한다. t값이 낮을수록 β값도 낮아진다.)
L2는 변수 선택이 불가능한 대신 미분으로 closed form solution을 구할 수 있으며, 변수 간 상관관계가 높을 때도 예측 성능이 좋다. L1보다 조금 빠르다.
L1은 변수 선택이 가능하나 미분이 불가능하며, 변수 간 상관관계가 높다면 예측력이 떨어진다. L2보다 조금 느리다.
ElasticNet: L1과 L2를 결합한 방식. L1 정규화에 L2 제한을 가하여, 1값과 2값을 조절해 가장 예측력이 좋은 값으로 설정한다.


데이터 스케일링(scaling)이란 무엇인가요?
개별 피처의 크기를 모두 같은 단위로 변경하는 것. 피처 스케일의 차이가 클 경우, 단위가 큰 피처가 더 중요하게 연산될 수 있기 때문에 필요하다. 크게 표준화(standardization)와 정규화(normalization)로 나눌 수 있다.
표준화: 피처를 정규 분포(평균 0, 분산 1)로 변환
정규화(normalization): 값들을 전부 [0, 1] 이내로 변환
Min-max scaling: 최소값을 0, 최대값을 1로 놓고 스케일링


불균형한 데이터를 어떻게 사용할 건가요?
언더샘플링 (undersampling): 더 큰 분포의 데이터를 줄이는 것
계산 시간이 감소하지만 정보 손실.
예) random undersampling, tomek links, condensed NN, OSS (tomek links + CNN)
오버샘플링 (oversampling): 더 작은 분포의 데이터를 늘리는 것
정보 손실 없고 정확도가 보다 높지만 overfitting 가능성이 있고 노이즈에 민감하며 계산 시간 증가.
예) resampling, SMOTE, Borderline-SMOTE, ADASYN


KNN과 K-means의 차이점에 대해 설명하세요.
KNN: 분류 (지도학습). Target label이 존재함.
K-means: 군집화 (비지도학습). Label 없는 데이터와 threshold만이 주어짐.


K-means나 KNN에 맨해튼 거리를 사용하지 못하는 이유는 무엇인가요?
해당 학습은 centroid를 중심으로 군집화하는 알고리즘이며, centroid는 유클리디안 거리를 사용하는 지표.

K-means의 대표적 단점은 무엇인가요?
시작 지점이 random하게 지정되기에 최적 값이 달라질 수 있다. (local minima)
K값에 따라 성능이 달라진다.
노이즈가 많은 경우 효과적이지 않다.

K-means에서 최적의 k값은 어떻게 구할 수 있을까요?
Elbow (inertia): SSE vs. k 그래프를 활용해 elbow의 x값 찾기
실루엣 계수: 각 군집 간 거리가 얼마나 효율적으로 분류되었는가를 나타냄. [0, 1] 내의 값으로, 1에 가까울수록 최적화가 잘 되어 있다 (즉, 더 효율적으로 분류되어 있다).

Local Minima와 Global Minima에 대해 설명해주세요. 특히 k-means clustering에서, local minima가 왜 중요한가요? Local minima 문제가 생겼다는 걸 어떻게 알 것이며, 어떻게 대처하나요?
Local minima는 주변 값들과 대조했을 때 최적의 결과를 도출하는 값.
Global minima는 모든 값들과 대조했을 때 최적의 결과를 도출하는 값.
K-means clustering 분석을 할 때, 비용 함수는 minima에 도달할 때까지 최적화를 계속할 것이다. Minima에 다다랐을 때, 군집화가 최적화되었다고 판단한다.
최적 값을 비교적 빨리 찾았거나, 시작 값에 변화를 줄 때 최적 값이 다르게 나타난다면, local minima 문제를 겪고 있을 확률이 높다. K-means clustering 분석에서 해당 문제를 해결하려면 여러 k값을 시도하며 가장 적은 비용을 리턴하는 값을 최적 값이라고 판단한다. 다른 분석(예: GD)에서는 learning rate를 알맞게 조절한다.


샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?
샘플링은 모집단에서 표본을 추출하는 것으로, 모집단을 향한 추측을 할 수 있도록 함.
리샘플링은 추출한 표본에서 다시 샘플링을 여러 번 시도하여 통계량의 변화를 확인하는 것. 종류로는 cross validation의 k-fold와 bootstrapping이 있다.
리샘플링의 장점은 정규성 등 샘플의 가정이 필요 없다는 것이며, 정확성이 향상된다.
Bootstrapping: 샘플링한 m개의 데이터 D가 있다면, D에서 데이터를 random하게 하나씩 골라 똑같은 m개의 새로운 데이터 D’를 만든다. 이 때, 추출하며 원 샘플 D에 같은 데이터를 replace한다.
K-fold와 bootstrapping의 차이점은 k-fold는 replacing 없이 추출을 하고, bootstrapping은  replacing을 하며 추출을 하는 것이다. 또한 cross validation은 모델 성능 예측에 강점을 보이며, bootstrapping은 분포 등 불확실한 통계 관련 parameter를 확인하는 데 사용된다.


차원의 저주에 대해 설명해주세요. 거리 및 유사도 지표에 어떤 식으로 영향을 미치나요?
피처의 차원이 커지면서 발생하는 문제 (피처의 수가 데이터 수보다 많아질 때). 차원이 커지면, 공간이 커지면서 차원 내 데이터도 sparse해지고, 데이터가 어디에 존재하는지 locate하기 어려움. 또한 메모리를 불필요하게 많이 차지하게 됨.
kNN 알고리즘에 특히 치명적. 데이터 간 거리가 점점 늘어남에 따라, 거리를 기반으로 이웃 군집을 설정하는 kNN 알고리즘의 분류 성능에 큰 타격을 입힘.
다른 알고리즘을 사용하거나 차원 축소가 필요.


차원 축소의 중요성은? 차원 축소 기법으로 보통 어떤 것들이 있나요?
차원 축소는 메모리 최적화, 다중공선성 제거, 시각화 발전, 차원의 저주 방지에 큰 도움이 된다.
차원 축소는 피처 선택과 피처 추출로 나눌 수 있음. 피처 선택이란 다중공선성 등 불필요한 피처를 제거하는 것이며, 피처 추출이란 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것.
피처 선택의 장점은 선택된 피처의 해석이 용이하다는 것. 단점은 피처간 상관관계 고려 어려움. 피처 추출의 장단점은 피처 선택의 장단점의 정반대.
피처 선택 기법: filtering (통계 분석으로 최적의 feature subset 선택; t-test, chi-squared test 등), wrapper (feature subset을 계속 만들어 반복적으로 test; forward greedy, backward greedy 등)
피처 추출 기법: PCA, SVD, NMF, LDA


PCA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요? PCA와 SVD의 관계성에 대해 설명해주세요.
PCA(Principal Component Analysis)는 데이터의 공분산 행렬을 기반으로 고유벡터를 생성하고 이렇게 구한 고유 벡터에 입력 데이터를 선형 변환하여 차원을 축소하는 방법이다.
SVD(Singular Value Decomposition)는 PCA와 유사한 행렬 분해 기법을 사용하나 정방 행렬(square matrix)를 분해하는 PCA와 달리 행과 열의 크기가 다른 행렬에도 적용할 수 있다. 차원 축소에도 사용하고, 행렬 재구현에 특히 뛰어나기에 이미지 처리에도 좋다. 예측하는 데에는 그리 뛰어나지 않으며, data sparsity에 취약하다.
LDA(Linear Discriminant Analysis)는 지도학습에서 사용하는 차원 축소 기법 중 하나로 특정 공간상에서 군집 클래스를 명확히 분리시키는 것에 집중한다. 즉, 분류하기 쉽도록 클래스 간 분산을 최대화하고 클래스 내부의 분산은 최소화하는 방식을 말한다.


PCA는 무엇인가요? PCA를 쓸 만한 문제들은 무엇이며, 단점은 무엇인가요?
비지도 변환의 하나로, 데이터의 분산을 유지할 수 있는 (즉, 정보를 가장 덜 손실하는) vector를 찾는 분석. 차원 축소 기법 중 하나로, 분산이 최대인 축을 찾은 후, 분산을 유지할 수 있는 축을 추가로 찾는다.
공간 데이터를 많이 소요하는 문제나 여러 차원의 피처들을 시각화할 때 사용한다.
단점: 각자 다른 스케일에 영향을 받으며, 변수 간 상관관계가 없을 경우 차원 축소 대신 분산 순으로 나열밖에 하지 않는다. 또한 차원 축소를 실행했기 때문에 특성이 혼합되어 시각화된 축의 의미를 파악하기 어렵다.


PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?
PCA는 입력 데이터의 공분산 행렬을 기반으로 eigenvector를 생성하고 입력 데이터를 선형 변환하여 차원 축소를 실행한다. 즉, 입력 데이터의 피처 역시 압축되기에 데이터 압축 기법이라고 볼 수 있다.
또한 eigenvalue가 가장 큰 (데이터의 분산이 가장 큰) 순서대로 주성분 벡터를 추출하기 때문에, 피처의 설명도를 순차적으로 나열할 수 있어 노이즈 제거 기법으로도 볼 수 있다.


feature vector란 무엇일까요?
양적 feature들을 나타내는 n차원의 벡터. Feature space는 feature vector의 vector space를 말한다. Feature 변수가 N개라면 feature space는 N차원이 된다.


SVM은 왜 반대로 차원을 확장시키는 방식으로 동작할까요? 거기서 어떤 장점이 발생했나요? SVM의 장점은 무엇인가요?
SVM이란 초평면을 활용해 데이터를 분류하는 방식으로, 선형적인 분리가 어려운 상황에서 차원을 확장시켜 면으로 분리하며 더 잘 분류할 수 있음. 군집 사이에 초평면으로 분류를 시행하며 경계에 support vector를 두고 사이 margin을 최대화시키는 방식으로 분류.
장점: 선형, 비선형 문제에 모두 사용 가능. 분류, 회귀 문제에 모두 사용 가능. 수치 및 범주 예측에 효과적. overfitting을 방지하고 오류 데이터에 민감하지 않음. 
단점: 연산이 느리고 메모리 할당량 높음. 여러 개의 hyperparameter 조합 테스트를 필요로 함.


SVM을 fit 하기 전에 차원 축소를 하는 것이 좋은가요? 이유는 무엇인가요?
때에 따라 다르다. 피처 수가 관측 데이터 수에 비해 더 많다면 (차원의 저주), 차원 축소 후 SVM을 fit하는 것이 더 좋을 것이다.


Random Forest와 SVM을 각각 언제 쓸 것인지 설명해주세요.
Random Forest: 데이터의 수가 많거나 데이터의 형태(양적, 질적)가 일정하지 않을 경우 (혹은, 데이터 전처리가 되어 있지 않은 경우), 모델링을 빠른 시간 내에 완수해야 할 경우, 다중 분류를 해야 할 경우
SVM: 스케일링 되어 있고 양적 데이터 (혹은 one-hot encode된 질적 데이터)의 binary 분류 문제의 경우, 메모리 capacity 제한이 널널한 경우
SVM은 조건이 까다롭지만, 조건이 충족되는 경우에는 거리 metric이 중요하다는 가정 하에 Random Forest보다 나은 성능을 보인다.


좋은 모델의 정의는 무엇일까요?
현재 데이터를 잘 설명하며, 미래 데이터에 대한 예측 성능이 좋은 모델. 현재 데이터를 잘 설명하려면 training set의 error를 최소화해야 하며, 예측 성능이 좋으려면 bias와 variance를 낮춰야 한다.
물론 좋은 모델도 중요하지만, 좋은 데이터가 더 중요하다 (전처리가 중요한 이유).
어느 상황에서나 항상 좋은 성능을 내는 모델은 없다.


꽤 오래된 방법인 나이브 베이즈가 좋은 모델이 아닌 이유는? 반대로 장점은 무엇일까요? 스팸메일 모델에 어떻게 나이브 베이즈를 써서 더 발전시킬 수 있을까요?
나이브 베이즈: 모든 피처들이 독립적이고 상관성이 없다고 가정. 데이터의 정보와 클래스의 사전 정보를 결합해 베이즈 정리를 이용하여 데이터를 분류하는 알고리즘.
장점: 연산 속도가 매우 빠르다. 노이즈나 결측 데이터가 있어도 수행할 수 있다.
단점: 실생활에서 피처들이 이 가정에 들어맞는 경우는 거의 없다.
공분산 행렬을 단위 행렬로 변환해 피처 간 상관관계를 없애는 방식으로 발전시킬 수 있다.


로지스틱 회귀는 무엇인가요? 스팸 필터에 로지스틱 회귀를 많이 사용하는 이유는 무엇일까요?
로지스틱 회귀는 시그모이드 함수를 활용한 회귀를 바탕으로 데이터가 특정 범주에 속할 확률을 [0, 1] 범주 내 값으로 예측하고 해당 확률에 따라 분류해주는 지도학습 알고리즘이다.
스팸 필터는 분류 문제다. 입력값에 무관하게 일정 범위 내에서 확률을 예측하기에 해당 문제에 적절하다.


선형 모델의 단점은? 정규화를 예로 들어 설명해 보세요.
모델이 선형이라고 가정하며, 연속적 값에만 적용 가능하다. 독립 변수가 증가할수록 분산도 증가하기 때문에 overfitting이 발생하게 되며, 이를 막기 위해 L2 정규화를 사용해 복잡도를 제어할 수 있다.


Decision tree는 무엇인가요? 장단점은? Pruning하는 방식은 무엇인가요?
분류와 회귀에 둘 다 쓰이며, 데이터를 두고 classification이나 threshold의 기준을 만들어 나누어 가는 방식이다.
회귀 나무를 나누는 기준은 Information gain이 가장 높은 attribute를 루트 노드에 가장 가깝게 두도록 하는 것이다. (상위 노드의 엔트로피와 하위 노드의 엔트로피의 차)
분류 나무를 나누는 기준은 불순도 측정 지표를 최소화시키는 쪽이다.
장점: 모델의 해석이 직관적이고 쉽다. 모든 결과를 고려한다.
단점: 데이터의 조그마한 변화에도 영향을 크게 받는다. 평균이나 다수결에 의거하여 예측을 수행하기 때문에 타 지도학습 모델보다 예측력이 떨어진다.
Pruning이란 과도한 children node 생성으로 overfitting이 된 decision tree의 가지를 줄여줌으로써 overfitting을 방지하는 것을 의미한다.
사전 가지치기: validation set을 통해 미리 depth나 sample threshold 지정
검증 데이터를 활용한 사후 가지치기 (분리 전 오분류 개수와 분리 후 오분류 개수를 비교하고 판단; 분리 전이 더 적다면 prune)


불순도 측정 지표에 대해 알고 있는 것이 있나요?
엔트로피: 정보량. 높을수록 정보량이 많고, 확률은 낮다. 로그를 취한다. 전부 같은 분류의 데이터가 있다면 엔트로피는 0, 정확히 반반이라면 엔트로피는 1이다.
지니 계수: 정답이 아닌 다른 라벨이 선택될 확률. 전부 같은 분류의 데이터가 있다면 지니는 0, 정확히 반반이라면 지니는 1이다.
엔트로피는 로그를 사용하기 때문에 지니 계수보다 연산이 느리다. 반면 엔트로피 커브가 좀 더 완만한 기울기를 가지고 있기에, 분류 성능 자체는 엔트로피가 상대적으로 나을 수 있다.


Random Forest는 무엇이며, 장점은 무엇인가요?
Decision tree를 bagging한 앙상블 모델. 주어진 feature들을 무작위로 선택해 tree를 만들어 분석하고, 이를 여러 번 반복한다.
장점: Overfitting을 방지하는 데 효과적. Scale 변화가 불필요하며, 결측치 처리에도 강함. 분류와 회귀 모두 사용 가능.
단점: 메모리 소모. Train data를 추가해도 성능 개선이 어려움.


50개의 작은 Decision tree는 큰 Decision tree보다 괜찮을까요? 왜 그렇게 생각하나요?
50개의 작은 Decision tree를 사용하는 앙상블 기법, 즉 Random Forest는 단일 Decision tree보다 error 교정에도 강하며 overfitting의 확률도 적다. 이상치에도 과도하게 반응하지 않는, 보다 robust한 모델이다.


데이터에 노이즈를 넣어 sensitivity를 계산하는 방식은 어떻게 생각하세요?
정규화와 비슷한 역할을 할 것으로 보인다. Overfitting을 줄이고 robustness를 향상시킬 것 같다.


선형 회귀 분석에 필요한 가정은 무엇인가요? 이 가정이 어긋난다면?
선형성: 독립변수와 종속변수는 선형 관계를 가지고 있음.
독립성: 독립변수 간의 상관관계가 없고 독립적.
등분산성: 분산이 같음, 즉 특정 패턴 없이 고르게 분포되어 있음.
정규성: 정규분포를 띄고 있음.
가정이 어긋난다면, 유의한 변수가 유의하지 않거나 그 반대의 경우가 나타날 수 있다.


(다중)공선성은 무엇이며, 어떻게 해야 하나요? 어떻게 다중공선성을 없앨 수 있나요?
다중 회귀 모델을 구축할 때, 2개 이상의 피처가 매우 높은 상관관계를 가지고 있는 것을 의미. 중복된 정보를 제공할 수 있다.
다중공선성이 문제되는 이유는, 독립성이 위배되어 회귀계수 값이 불안정해지며 표준 오차가 늘어나기 때문. 즉, 각 변수의 설명력이 약해진다.
다중공선성을 점검하려면 변수 간 상관관계를 조사하는 방법이나 VIF를 조사하는 방법을 사용해야 한다. VIF란 독립변수를 하나씩 종속변수로 지정하고 나머지 독립변수로 회귀 분석을 진행하는 것으로, 큰 값이 반환될 수록 다중공선성을 증명한다.
다중공선성을 없애려면 높은 상관관계에 있는 변수를 하나 없애거나, 변수들을 하나로 취합하는 방법이 있다.


토픽 모델링이란 무엇이며, LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)란 무엇인가요? 언제 쓰이며, 단점은 무엇인가요? (8)
토픽 모델링: NLP에서 토픽이라는 문서의 주제를 발견하기 위한 통계적 모델. 텍스트 마이닝 기법으로, 텍스트의 숨겨진 의미 구조를 발견하는 데 중점을 둠.
Truncated SVD는 SVD와 똑같으나 상위 n개의 특이값만 사용하는 축소 방법이다. 이 방법을 쓸 경우 원 행렬로 복원할 수 없다.
LSA는 잠재 의미 분석을 말하며, 주로 토픽 모델링에 자주 사용되는 기법이다. LSA는 DTM이나 TF-IDF 행렬에 Truncated SVD를 적용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 이끌어낸다. TF-IDF 기법에 비해 단어의 의미를 신경쓰며 연산이 빠르다는 장점이 있으나, 새로운 문서가 추가되면 연산을 다시 해야 한다는 단점이 있다. 또한 단어 순서는 신경쓰지 않는다.
LDA는 토픽 개수 n개를 정하고 모든 문장들의 단어를 n개의 토픽 중 하나로 분류한 후, 잘못 분류된 토픽은 다른 모든 토픽이 제대로 분류되었다고 가정하고 해당 문장이나 전체 문서에서 토픽별 분포를 기반해 다수 쪽으로 결정하는 토픽 모델링이다. 단어 순서는 신경쓰지 않는다.


텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?
토픽 모델링. 세부적으로는 LDA 사용.


최소제곱법(OLS regression)의 공식은 무엇인가요?
MSE를 최소화하는 회귀계수 β 계산:
LS=XTX-1XTy


Markov Chain은 무엇인가요?
Markov 성질이란 오직 이전 단계, 혹은 이전 n개의 단계에서의 결과값에만 영향을 받는 것을 뜻함.
Markov Chain은 이 Markov 성질을 따른 데이터에서 이산 시간 확률 과정을 바탕으로 연산하는 것.


연관규칙분석(Association Rule)의 Support, Confidence, Lift에 대해 설명해주세요.
연관규칙분석 (장바구니 분석): 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 사용.
평가 지표:
Support: 전체 거래 중 항목 A와 B를 동시에 포함하는 거래의 비율
Confidence: 항목 A를 포함한 거래 중 항목 A와 B를 동시에 포함하는 거래의 비율 (연관성의 정도 파악 가능)
Lift: A가 포함되지 않은 거래 중 B가 포함된 비율로부터, A가 포함된 거래 중 B가 포함된 비율로의 증가비 (서로 관련이 없을수록 Lift는 기본비인 1과 가까움)

머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?
접근 방식이 다르다. 머신러닝은 예측 성능을 높이는 데 집중하기 때문에, 다양한 피처를 사용하여 높은 예측률을 달성하는 것이 목표다. 통계적 접근방법은 분포와 가정을 통해 신뢰 가능하고 정교한 모델을 만드는 것을 주 목표로 한다.


인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?
딥러닝 이전의 인공신경망은 선형적 회귀나 분류 방식만을 택했기에 XOR 등 비선형 방식을 필요로 하는 문제를 해결하지 못했으나, 시그모이드 함수 등 비선형 방식과 오차역전파의 모델 업데이트 방식을 발전시켜 문제점을 극복했다.


여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야 하는 이유는 뭘까요?
Random Forest는 Decision Tree의 배깅 앙상블 알고리즘이기에, 각 서버를 하나의 Decision Tree로 병렬적이게 구성 가능하나, 인공신경망은 서버 하나하나가 모델의 특성을 모두 이해하는 end-to-end 구조로 직렬적이게 구성된다. 따라서 병렬적으로 여러 서버를 활용할 수 있는 Random Forest 방식을 사용한다.

Gradient Boosting 회귀 트리는 무엇인가요? XGBoost는 무엇인가요?
Gradient Boosting 회귀 트리는 Decision Tree의 앙상블 모델이나, 배깅 (Random Forest) 대신 부스팅 앙상블 모델이다. Iteration 수나 learning rate (오차 보정치) 등 hyperparameter 설정에 보다 민감하지만 더 높은 정확도를 제공해 줄 수 있다. 다만 훈련 시간이 길다. 분류와 회귀 문제에 모두 사용 가능하다.
XGBoost는 일반 GB에 비해 다음과 같은 장점들을 발전시킨 패키지로, 매우 각광받고 있다.
보다 빠른 수행 시간 (병렬 처리)
정규화 규제, early stopping 등 추가 기능을 통해 overfitting 방지 가능
자체적으로 결측치 처리


앙상블 방법엔 어떤 것들이 있나요?
보팅: 동일 샘플에 다른 모델들을 각자 적용시켜 각자 모델 결과값의 다수결(hard voting)이나 높은 확률(soft voting) 순으로 예측.
배깅: 동일 샘플을 여러 번 bootstrapping해서 같은 얕은 모델로 여러 번 학습한 후, 보팅(질적)이나 평균(양적)으로 결과 집계. 데이터가 적어도 underfitting이나 overfitting에서 비교적 자유로움. (RF)
부스팅: 이전 모델의 오답에 가중치를 부여해 다음 모델에 적용. 배깅에 비해 정확도가 높으나, 오답을 반복적으로 학습하기 때문에 overfitting의 위험성이 높고 이상치에 취약하며 속도 느림. (GBM)
스태킹: 여러 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델 만들기. Overfitting 방지를 위해 데이터를 쪼개서 일부만 학습한 후, 각 모델의 결과들을 취합헤 meta train set을 만들어 재학습.


Training 세트와 Test 세트를 분리하는 이유는? Validation 세트가 따로 있는 이유는? Test 세트가 오염되었다는 말의 뜻은?
Training set과 test set을 분리하지 않는다면 overfitting에 취약한 모델이 된다. 또한, validation set 없이 학습을 진행한다면 test set의 평가 지표에 따라 hyperparameter를 조절할 것이며, 그렇다면 오히려 test set의 특성에 overfitting될 위험이 있다. 즉, training set에서 학습을 진행하고, validation set을 평가해 hyperparameter를 조정하고, 완성된 모델을 test set에 적용한다.
Test set이 오염되었다는 뜻은 training set와 train set이 완벽히 분리되지 않아서 서로 영향을 주고 받거나, test set까지 포함해 정규화를 해 버려서 training set에 영향을 주는 등의 일을 말한다.


공분산(covariance)과 상관도(correlation)를 비교하세요. 
공분산은 두 개의 random 확률변수가 어떠한 상관성을 띄고 있는지 알려준다. (변수의 크기에 큰 영향을 받는다.)
상관도는 두 개의 변수가 어떠한 상관성을 띄는지, 얼마나 띄는지를 알려준다. (크기 영향 없음)


Continuous와 categorical 변수 사이의 correlation은 어떻게 계산하나요?
Categorical 변수가 binary일 때만 가능. 0과 1로 코딩하고 Pearson 상관계수를 계산 (점이연 상관계수)


PCA에 있어서 rotation이 꼭 필요할까요?
필수적이지는 않지만, 연산에 있어서 최적화를 시켜주고 해석력을 향상시켜 줌.


PCA 이외의 데이터 압축 기법을 알고 있나요? (27) t-SNE?
베이즈 정리를 머신러닝에 관련지어 설명하세요.
지표의 예측력이란 무엇인가요? 어떻게 계산하나요? (28)
회귀 모델에서 회귀 계수를 꼭 필요로 하나요? (29)
Model의 정확성이 더 중요할까요, performance가 더 중요할까요?
회귀 모델은 어떻게 train할 수 있을까요? 계수는 어떻게 해석하나요? (36)
분류 문제에서 알맞은 머신러닝 알고리즘을 찾는 방법은?
유저가 10만명, 아이템이 100만개 있습니다. 이 경우 추천 시스템을 어떻게 구성하시겠습니까? (16)
한 모델이 다른 모델보다 좋은지는 어떻게 규정하나요? (17)
알고리즘에 조그마한 변화를 주는 것이 주지 않는 것보다 낫다는 것은 어떻게 알 수 있나요? (25)
A/B 테스트란 무엇인가요? A/B 테스트의 장점과 단점, 그리고 단점의 경우 이를 해결하기 위한 방안에는 어떤 것이 있나요? (25+)
만약 A/B 테스트에서 기존 모델에 비해 새 모델의 metric이 매우 적은 차이로 더 나았고, p-value는 매우 적은 값이 나왔다면 모델을 바꾸는 것이 좋을까요?

Step-wise regression은 무엇인가요? Step-wise 기법에 대해 알고 있나요? (27)
Image processing과 머신러닝은 어떻게 연관되어 있나요?
머신러닝 알고리즘을 parallelize하는 tool로는 무엇이 있을까요? Parallelize란?
푸리에 변환에 대해 설명하세요.
데이터 마이닝과 머신러닝의 차이는 무엇인가요?
AI와 머신러닝의 차이는 무엇인가요?
Convex function과 Non-convex function의 차이는 무엇인가요?
최적화 기법중 Newton’s Method 방법에 대해 알고 있나요?
고유값(eigenvalue)와 고유벡터(eigenvector)에 대해 설명해주세요. 왜 중요할까요?
Maximal margin classifier란 무엇인가요? 해당 margin을 어떻게 처리할 수 있나요? (37)
Kernel은 무엇인가요? Kernel trick에 대해 설명해 보세요.
Kernel trick: 커널 함수를 사용하여 저차원 공간을 고차원 공간으로 매핑해주는 작업 (비선형 SVM 때 마진을 많이 남기는 초평면을 찾기 위해 사용)
알고 있는 kernel의 종류가 있나요? 어떻게 kernel을 선택하나요? (39)
새로운 credit risk scoring 모델이 잘 작동한다는 것을 어떻게 알 수 있을까요? (18)
N-gram에 대해 설명해 보세요. (19)
Artificial Neural Network는 무엇이며, back propagation은 무엇인가요? (41)

추천 시스템
추천 시스템에서 사용할 수 있는 유사도 지표는 무엇이 있을까요?
피어슨 상관계수
두 벡터 사이의 선형 상관관계를 찾음 (-1 ~ 1) for CF
특별히 어려운 계산이 없는 단순한 알고리즘 (무난무난)
모든 neighbor의 rating이 동등하게 “가치 있는” 것은 아닐 것이다. (베스트셀러는 누구나 다 좋아하지…)
가중치로 더 발전시킬 수 있을 듯. 분산이 많은 것에 가중치, 많은 데이터를 바탕으로 유사도가 높은 유저에 가중치…
코사인 유사도
IBCF에 좋음. 사용자의 평가값을 좌표로, 아이템을 하나의 차원으로 해서 두 벡터 사잇각을 구한다. (유사도가 높을수록 코사인값이 크다)
자카드 유사도
Binary 데이터일 경우; 교집합 / 합집합

MAB를 설명하고, 이를 측정할 수 있는 방법들 중 하나를 설명하세요.
Thompson Sampling에 대해 설명하세요. 어떻게 작동합니까? Initial state는?
이미 보유한 데이터를 Thompson Sampling에 새로 적용시키고 싶다고 했을 때, 초기 분포를 어떻게 지정하고 수정할까요?
User 베이스 추천 시스템과 Item 베이스 추천 시스템 중 단기간에 빠른 효율을 낼 수 있는 것은 무엇일까요?
성능 평가를 위해 어떤 지표를 사용할까요?
Serendipity와 intra-list similarity는 실제로 어떻게 평가받아야 하나요?
Retention-rate는 실제로 어떻게 평가받아야 하나요?
Explicit Feedback과 Implicit Feedback은 무엇일까요? Implicit Feedback을 어떻게 Explicit하게 바꿀 수 있을까요?
Matrix Factorization은 무엇인가요? 해당 알고리즘의 장점과 단점은?
SQL으로 조회 기반 Best, 구매 기반 Best, 카테고리별 Best를 구하는 쿼리를 작성해주세요
추천 시스템에서 KNN 알고리즘을 활용할 수 있을까요?
딥러닝을 활용한 추천 시스템의 사례를 알려주세요
두 추천엔진간의 성능 비교는 어떤 지표와 방법으로 할 수 있을까요? 검색엔진에서 쓰던 방법을 그대로 쓰면 될까요? 안될까요?
Cold Start의 경우엔 어떻게 추천해줘야 할까요?
고객사들은 기존 추천서비스에 대한 의문이 있습니다. 주로 매출이 실제 오르는가 하는 것인데, 이를 검증하기 위한 방법에는 어떤 것이 있을까요? 위 관점에서 우리 서비스의 성능을 고객에게 명확하게 인지시키기 위한 방법을 생각해봅시다.
분석 일반
좋은 feature란 무엇인가요. 이 feature의 성능을 판단하기 위한 방법에는 어떤 것이 있나요?
Feature importance 측정 방법으로는 무엇이 있나요?
각 고객의 웹 행동에 대하여 실시간으로 상호작용이 가능하다고 할 때에, 이에 적용 가능한 고객 행동 및 모델에 관한 이론을 알아봅시다.
고객이 원하는 예측모형을 두가지 종류로 만들었다. 하나는 예측력이 뛰어나지만 왜 그렇게 예측했는지를 설명하기 어려운 random forest 모형이고, 또다른 하나는 예측력은 다소 떨어지나 명확하게 왜 그런지를 설명할 수 있는 sequential bayesian 모형입니다. 고객에게 어떤 모형을 추천하겠습니까?
고객이 내일 어떤 상품을 구매할지 예측하는 모형을 만들어야 한다면 어떤 기법(예: SVM, Random Forest, logistic regression 등)을 사용할 것인지 정하고 이를 통계와 기계학습 지식이 전무한 실무자에게 설명해봅시다.
나만의 feature selection 방식을 설명해봅시다.
데이터 간의 유사도를 계산할 때, feature의 수가 많다면(예: 100개 이상), 이러한 high-dimensional clustering을 어떻게 풀어야할까요?


딥러닝 일반
딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?
딥러닝은 머신러닝의 일부로, 인공신경망을 사용하여 머신러닝 학습을 수행한다.
머신러닝은 데이터의 여러 특징 중 무엇을 추출할지 사람이 직접 분석하고 판단하나 (feature engineering), 딥러닝에서는 기계가 자동으로 특징을 추출한다. 따라서 보다 큰 데이터셋과 긴 학습시간을 필요로 한다.
주로 정형 데이터는 머신러닝, 비정형 데이터는 딥러닝을 사용한다.


Cost Function과 Activation Function은 무엇인가요?
Cost function(비용 함수): 예측 값과 실제 값의 차이에 대한 함수 (즉, 최소화해야 한다)
Activation function (활성화 함수): 보다 복잡한 데이터를 예측할 수 있도록 선형 모델을 비선형 모델로 변환해주는 함수. 선형 모델의 함수와 활성화 함수가 결합되어 비선형 모델이 된다. 여기서 가중치(weight)와 편향(bias)이 중요하게 작용하는데, 가중치는 활성화 함수의 기울기를 조절하며, 편향은 함수를 수평이동하여 데이터에 더 잘 맞도록 한다. 


알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)
Sigmoid: 입력을 0과 1 사이의 값으로 변환. 입력 값의 절대값이 높을수록 기울기를 소실하는 gradient vanishing 문제가 있음.
Tanh: 입력을 -1과 1 사이의 값으로 변환. 마찬가지로 gradient vanishing 문제.
ReLU: max(0, x). Gradient vanishing 문제가 없고 효율과 성능이 뛰어나나, 음수일 경우 Dead ReLU.
Leaky ReLU: max(0.01x, x). 좋은 성능을 유지하며 Dead ReLU 문제 해결.


딥러닝에서 overfitting일 경우 어떻게 대처해야 할까요?
Early stopping: validation loss가 높아지는 시점에서 training epoch 추가를 중단.
정규화: L2나 L1 정규화로 비용함수의 weight의 크기에 페널티 부과.
Data augmentation: (특히 이미지 데이터일 때) 데이터의 수가 적을 경우, 인위적으로 mirroring 등의 변화를 주어 훈련 데이터의 수를 증가.
Noise robustness: 노이즈나 이상치의 면역을 늘릴 수 있도록 데이터나 weight에 의도적인 노이즈 부여.
Batch-normalization: 활성화 함수의 활성화값 (출력값)을 정규화. 각 은닉층에서 정규화를 하며 input 분포가 일정해지고, learning rate의 영향을 덜 받게 된다. 결과적으로 학습속도가 빨라진다.


Batch Normalization의 효과는?
Mini-batch 단위로 입력 데이터의 평균을 0, 분산을 1로 표준화시키는 것. 기울기 소실 문제에서 자유로워 learning rate를 크게 설정할 수 있고, 따라서 학습 속도도 빨라진다. 또한 초기 가중치 값에 크게 의지하지 않아도 된다 (어차피 표준화되기 때문)


Hyperparameter는 무엇인가요?
사용자가 직접 세팅해주는 값. 튜닝 기법으로는 Grid Search, Manual Search, Random Search 등.


요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?
Vanishing gradient 문제를 해결하기 위해서. 다만 ReLU도 Dead ReLU 문제가 있기 때문에 Leaky ReLU로 이를 보완할 수 있다.


Gradient Descent에 대해서 쉽게 설명한다면?
특정 함수의 최소값(혹은 최적값)을 찾기 위해 미분하여 기울기를 구하고 경사 하강 방향으로 이동해 parameter 값을 도출.
단점: learning rate에 따라 결과값이 다르거나 결과 도출이 불가능할 수 있다. Local minima 문제에서 자유롭지 못하다.


hyperparameter 튜닝 기법의 차이점은?
Grid search: hyperparameter의 모든 조합들이 시도되고 검증됨. 정확하나, 당연히 hyperparameter 개수가 많을수록 튜닝에 차질. 10-fold CV까지 행할 경우 iteration 더욱 증가.
Random search: 주어진 iteration 수에 따라 random하게 조합을 설정해 검증. Grid search보다는 정확성이 떨어지지만 꽤 정확한 결과를 도출하며 훨씬 더 빠른 시간 내에 연산 가능.
Bayesian optimization: 이전 hyperparameter 조합에 근거해 다음 조합 설정. 각 조합 연산 설정에는 시간이 조금 더 들지만, iteration 수가 급감하며 더욱 빠르게 연산 가능.


SGD(Stochastic Gradient Descent)는 무엇인가요?
Loss function을 최소화하기 위해 MF의 P, Q 두 행렬을 동시에 최적화. 편미분을 통해서 권장 p, q값을 업데이트하고, 여러 iteration을 통해 loss function을 최소화한다. Overfitting 방지를 위해 L2 정규화 사용. 또한 전체평균을 제거하고 사용자의 평가경향을 수정해야 한다. (즉 p, q, bi, bj 동시 수정)
장점: 유연하며, 여러 loss function에 적용 가능
단점: 느린 속도, 미관측 항목은 다루기 어려움


Grid Search 등 hyperparameter 튜닝 기법의 차이점은?
Mini-batch를 작게 할 때의 장단점은?
마지막으로 읽은 논문은 무엇인가요? 설명해주세요.


Back Propagation에 대해서 쉽게 설명 한다면?
Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는? GD가 Local Minima 문제를 피하는 방법은? 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?
RMSprop, Adam에 대해서 아는대로 설명한다면?
딥러닝할 때 GPU를 쓰면 좋은 이유는?
TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?
뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?
통계
A/B Test 등 현상 분석 및 실험 설계 상 통계적으로 유의미함의 여부를 결정하기 위한 방법에는 어떤 것이 있을까요?
Long-tailed 분포란 무엇인가요? 예를 들어 설명해 주세요. 왜 회귀와 분류 문제에 중요할까요?
중심극한정리는 무엇이며, 왜 유용한 걸까요?
Type 1 error와 Type 2 error는 어떻게 다른가요? 검정력(Statistical power)과 무슨 관계가 있나요?
데이터의 selection bias는 무엇인가요? 왜 중요한가요? 왜 결측치를 채워 넣는 것이 selection bias를 더 악화시킬 수도 있을까요?
실험 데이터가 행동에 미치는 영향을 예를 들어 설명해 주세요.
실험 데이터는 관찰 데이터와 어떻게 다른가요?
결측치가 있을 경우 채워야 할까요? 그 이유는 무엇인가요? 결측치에 평균값(mean)을 채워 넣는 건 어떨까요?
결측치를 처리하는 방법에는 그대로 놔두기, 제거하기, 특정 값으로 채우기, 예측 값으로 채우기가 있다.
그대로 놔두기: 결측값을 잘 고려하는 모델 (xgboost)은 괜찮으나, 고려하지 않는 모델 (선형 회귀)의 경우 결측치 때문에 학습이 어렵거나 불가능하다.
데이터 제거: 만약 피처가 많을 경우, 단 하나의 결측치 때문에 모든 특성을 제거해야 하는 일이 벌어진다. (전체 데이터의 수가 많고 피처 수가 적다면 괜찮을 방법일 수도.)
특정 값 채우기: 결측치가 많을수록 무의미. Context에 따라 0 등의 값으로 채우는 건 괜찮은 방법일 수 있다.
예측 값 채우기 (평균, 중앙값, kNN 등): 이 역시 결측치가 많을수록 무의미. Context에 따라 다름. 분포를 고려해야 하는 추가적인 확인이 필요하다.


이상치(outlier)를 판단하는 기준은 무엇인가요?
IQR (Q3 - Q1) 기법: Q1 - 1.5*IQR 보다 낮거나, Q3 + 1.5*IQR 보다 높다면 이상치.
Z-score 사용: 가우시안 분포일 때만 사용 가능. 특정 z-score 값보다 높거나 낮다면 이상치.


이상치는 어떻게 처리해야 하나요?
Context에서 벗어난 값일 경우 삭제.
극도로 높거나 낮은 값일 경우, context에 맞지 않다면 삭제.
삭제가 불가능한 이상치일 경우:
다른 모델 사용 (선형 모델 대신 비선형 모델로?)
데이터 정규화 (normalization)로 스케일링
Random forest 등 이상치에 영향을 덜 받는 모델 사용


콜센터 통화 지속 시간에 대한 데이터가 존재합니다. 이 데이터를 코드화하고 분석하는 방법에 대한 계획을 세워주세요. 이 기간의 분포가 어떻게 보일지에 대한 시나리오를 설명해주세요.
Administrative 데이터와 그 단점, 그리고 실험적 방법과 그 단점을 설명해주세요.
고객들의 사진 포스팅 수가 10월에 특히 많아졌다면 그 이유는 무엇일까요?
출장을 위해 비행기를 타려고 합니다. 당신은 우산을 가져가야 하는지 알고 싶어 출장지에 사는 친구 3명에게 무작위로 전화를 하고 비가 오는 경우를 독립적으로 질문해주세요. 각 친구는 2/3로 진실을 말하고 1/3으로 거짓을 말합니다. 3명의 친구가 모두 “그렇습니다. 비가 내리고 있습니다”라고 말했습니다. 실제로 비가 내릴 확률은 얼마입니까?
Lift, KPI, robustness, model fitting, 실험 디자인, 80/20 규칙에 대해 설명해주세요.
Quality assurance와 six sigma에 대해 설명해주세요.
가우시안 분포나 log-normal 분포를 띠지 않는 예를 들어보세요.
Root cause 분석은 무엇인가요? "상관관계는 인과관계를 의미하지 않는다"라는 말이 있습니다. 설명해주실 수 있나요?
평균(mean)과 중앙값(median)중에 어떤 케이스에서 무엇을 써야 할까요?


대수의 법칙(Law of Large Numbers)이란 무엇인가요?
데이터가 많아질수록 sample mean과 sample variance는 population mean과 population variance에 가까워진다.


필요한 표본의 크기를 어떻게 계산합니까?
샘플링을 할 때 어떤 bias에 주의해야 하나요? Bias는 어떻게 조율할 수 있을까요?
Confounding variable이란 무엇인가요?
독립변수가 아니면서 종속변수에 큰 영향을 미치는 변수.


p-value란?
Type 1 Error를 범할 확률, 즉 null hypothesis(H0)를 잘못 reject할 확률. 주로 significance level (ɑ)을 정하곤 하는데, p-value가 ɑ보다 높다면 H0 유지, 낮다면 H0을 reject한다.


p-value는 요즘 시대에도 여전히 유효할까요? 언제 p-value가 실제를 호도하는 경향이 있을까요?
R square의 의미는 무엇인가요? 고객에게는 어떻게 설명하실 예정인가요?
엔트로피(entropy)에 대해 설명해주세요. 가능하면 Information Gain도요.
하지만 또 요즘같은 빅데이터(?)시대에는 정규성 테스트가 의미 없다는 주장이 있습니다. 맞을까요?
어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?
“likelihood”와 “probability”의 차이는 무엇일까요?
Prior probability, likelihood, marginal likelihood를 나이브 베이즈와 관련지어 설명하세요.
모수가 매우 적은 (수십개 이하) 케이스의 경우 어떤 방식으로 예측 모델을 수립할 수 있을까요?
베이지안과 프리퀀티스트간의 입장차이를 설명해주실 수 있나요?
확률 모형과 확률 변수는 무엇일까요?
누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요
베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포 / 디리클레 분포에 대해 설명해주세요. 혹시 연관된 분포가 있다면 연관 관계를 설명해주세요
ANOVA는 언제 사용하나요?
조건부 확률은 무엇일까요?
공분산과 상관계수는 무엇일까요? 수식과 함께 표현해주세요
신뢰 구간의 정의는 무엇인가요? 어떻게 계산하나요? 점추정과의 차이는 무엇인가요?
로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.
Factor analysis와 PCA의 차이는 무엇인가요?
자연어 처리
One Hot 인코딩에 대해 설명해주세요
문장에서 “Apple”이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요?
뉴스 기사에 인용된 텍스트의 모든 항목을 어떻게 찾을까요?
음성 인식 시스템에서 생성된 텍스트를 자동으로 수정하는 시스템을 어떻게 구축할까요?
잠재론적, 의미론적 색인은 무엇이고 어떻게 적용할 수 있을까요?
뉴스 기사를 주제별로 자동 분류하는 시스템을 어떻게 구축할까요?
Stop Words는 무엇일까요? 이것을 왜 제거해야 하나요?
영화 리뷰가 긍정적인지 부정적인지 예측하기 위해 모델을 어떻게 설계하시겠나요?
TF-IDF 점수는 무엇이며 어떤 경우 유용한가요?
PageRank 알고리즘은 어떻게 작동하나요?
Word2Vec의 원리는?
그 그림에서 왼쪽 파라메터들을 임베딩으로 쓰는 이유는?
그 그림에서 오른쪽 파라메터들의 의미는 무엇일까?
남자와 여자가 가까울까? 남자와 자동차가 가까울까?
번역을 Unsupervised로 할 수 있을까?
POS 태깅은 무엇인가요? 가장 간단하게 POS tagger를 만드는 방법은 무엇일까요?
dependency parsing란 무엇인가요?
Translate 과정 Flow에 대해 설명해주세요
한국어에서 많이 사용되는 사전은 무엇인가요?
Regular grammar는 무엇인가요? regular expression과 무슨 차이가 있나요?
RNN에 대해 설명해주세요
LSTM은 왜 유용한가요?
영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요?
시각화
"신규/재방문자별 지역별(혹은 일별) 방문자수와 구매전환율"이나 "고객등급별 최근방문일별 고객수와 평균구매금액"와 같이 4가지 이상의 정보를 시각화하는 가장 좋은 방법을 추천해주세요
구매에 영향을 주는 요소의 발견을 위한 관점에서, 개인에 대한 쇼핑몰 웹 활동의 시계열 데이터를 효과적으로 시각화하기 위한 방법은 무엇일까요? 표현되어야 하는 정보(feature)는 어떤 것일까요? 실제시 어떤 것이 가장 고민될까요?
파이차트는 왜 구릴까요? 언제 구린가요? 안구릴때는 언제인가요?
히스토그램의 가장 큰 문제는 무엇인가요?
워드클라우드는 보기엔 예쁘지만 약점이 있습니다. 어떤 약점일까요?
어떤 1차원값이, 데이터가 몰려있어서 직선상에 표현했을 때 보기가 쉽지 않습니다. 어떻게 해야할까요?


서비스 구현 (python, javascript, ...)
당장 10분안에 사이트의 A/B 테스트를 하고 싶다면 어떻게 해야 할까요? 타 서비스를 써도 됩니다.
신규 방문자와 재 방문자를 구별하여 A/B 테스트를 하고 싶다면 어떻게 해야 할까요?
쇼핑몰의 상품별 노출 횟수와 클릭수를 손쉽게 수집하려면 어떻게 해야 할까요?
R의 결과물을 python으로 만든 대시보드에 넣고 싶다면 어떤 방법들이 가능할까요?
여러 웹사이트를 돌아다니는 사용자를 하나로 엮어서 보고자 합니다. 우리가 각 사이트의 웹에 우리 코드를 삽입할 수 있다고 가정할 때, 이것이 가능한가요? 가능하다면, 그 방법에는 어떤 것이 있을까요?

비즈니스 / 커뮤니케이션
고객이 궁금하다고 말하는 요소가 내가 생각하기에는 중요하지 않고 다른 부분이 더 중요해 보입니다. 어떤 식으로 대화를 풀어나가야 할까요?
현업 카운터 파트와 자주 만나며 실패한 분석까지 같이 공유하는 경우와, 시간을 두고 멋진 결과만 공유하는 케이스에서 무엇을 선택하시겠습니까?
고객이 질문지 리스트를 10개를 주었습니다. 어떤 기준으로 우선순위를 정해야 할까요?
오프라인 데이터가 결합이 되어야 해서, 데이터의 피드백 주기가 매우 느리고 정합성도 의심되는 상황입니다. 우리가 할 수 있는 액션이나 방향 수정은 무엇일까요?
동시에 여러개의 A/B테스트를 돌리기엔 모수가 부족한 상황입니다. 어떻게 해야할까요?
고객사들은 기존 추천서비스에 대한 의문이 있습니다. 주로 매출이 실제 오르는가 하는 것인데, 이를 검증하기 위한 방법에는 어떤 것이 있을까요?
위 관점에서 우리 서비스의 성능을 고객에게 명확하게 인지시키기 위한 방법을 생각해봅시다.

https://github.com/boostcamp-ai-tech-4/ai-tech-interview/tree/main/answers
https://yongwookha.github.io/MachineLearning/2021-01-29-interview-question
