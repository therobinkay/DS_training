# 머신러닝
### 머신러닝의 정의는 무엇인가요?
알고리즘을 통해 시스템을 발전시켜 세부적인 명령 없이도 자동적으로 습득하고, 패턴을 찾고, 결정을 내리는 기계를 만들어 내는 것.


### 지도학습과 비지도학습의 차이가 무엇인지 상세한 예를 들어 설명하세요.
지도학습이란 label된 training data로부터 모델을 추측하는 것이다. Target label을 바탕으로 변수 간 상관관계를 파악하거나 예측 모델을 만든다. 종류로는 SVM, 인공지능망, 선형회귀, 로지스틱 회귀, XGBoost 등이 있다.
예) 집 평수에 근거하여 가격 예측, 검색 엔진 결과의 관계성 예측 등
비지도학습이란 label이 없는 data에서 숨겨진 상관관계나 패턴을 파악하는 것이다. Target label은 존재하지 않는다. 종류로는 clustering, PCA, SVD 등이 있다.
예) 투표에 의거해 유권자들 분류, 이미지 세분화 등


### Training 세트와 Test 세트를 분리하는 이유는? Validation 세트가 따로 있는 이유는? Test 세트가 오염되었다는 말의 뜻은?
Training set과 test set을 분리하지 않는다면 overfitting에 취약한 모델이 된다. 또한, validation set 없이 학습을 진행한다면 test set의 평가 지표에 따라 hyperparameter를 조절할 것이며, 그렇다면 오히려 test set의 특성에 overfitting될 위험이 있다. 즉, training set에서 학습을 진행하고, validation set을 평가해 hyperparameter를 조정하고, 완성된 모델을 test set에 적용한다.
Test set이 오염되었다는 뜻은 training set와 train set이 완벽히 분리되지 않아서 서로 영향을 주고 받거나, test set까지 포함해 정규화를 해 버려서 training set에 영향을 주는 등의 일을 말한다.


### Cross Validation은 무엇이고 어떻게 해야하나요?
통계적 분석이 독립적인 데이터셋을 분석할 때 얼마나 일반화되는지 조사하는 model validation 방식. 보통 예측 모델의 성능을 조사할 때, overfitting을 방지하고 독립적인 데이터셋에 얼마나 잘 반응하는지 보기 위해 training phase에서 행해진다.
train set을 다시 한 번 train set + validation set으로 나누어 학습 중 검증과 수정을 하는 것을 의미. 적은 데이터로도 정확도를 향상시킬 수 있지만, iteration 횟수가 많아 훈련과 평가에 시간이 오래 걸리는 편.
종류로는 크게 k-fold, stratified k-fold, leave-one-out (LOOCV) 등이 있다.
K-fold: 가장 일반적. 데이터를 k개로 나누고 하나를 validation set으로 지정해, k번 iterate하며 train으로 학습하고 validation으로 테스트한 후 평균을 구한다.
Stratified k-fold: 주로 분류 문제에 사용. Label의 분포가 불균형할 때 분포를 고려해 validate.
LOOCV: 하나의 데이터를 제외하고 나머지를 모두 학습에 사용. K-fold보다 bias는 낮으나 variance가 높을 수 있음. 학습된 train set이 전체 데이터셋과 유사하고 평균의 분산값은 결과값의 correlation이 높을 수록 증가하기 때문.
(보통은 5-fold, 10-fold를 사용. Bias-variance trade-off에서 좋은 성적을 보이기에.)


### Bias, variance, bias-variance tradeoff에 대해 각각 설명하고, overfitting과 underfitting이 이에 어떤 영향을 끼치는지 관련지어 설명하세요.
Bias(편향): 리턴 값들이 최적 값에서 얼마나 떨어진 곳에 집중되어 있는가?
Variance(분산): 리턴 값들이 얼마나 넓게 퍼져 있는가?
Bias가 높을수록 underfitting, variance가 높을수록 overfitting.
Bias와 variance는 trade-off 관계에 있으며, 이 둘 모두가 적절히 낮을 때 좋은 모델이라 할 수 있다.


좋은 모델의 정의는 무엇일까요?
현재 데이터를 잘 설명하며, 미래 데이터에 대한 예측 성능이 좋은 모델. 현재 데이터를 잘 설명하려면 training set의 error를 최소화해야 하며, 예측 성능이 좋으려면 bias와 variance를 낮춰야 한다.
물론 좋은 모델도 중요하지만, 좋은 데이터가 더 중요하다 (전처리가 중요한 이유).
어느 상황에서나 항상 좋은 성능을 내는 모델은 없다.


머신러닝에서 overfitting일 경우 어떻게 대처해야 할까요?
모델을 단순화. 고려된 관련 피처의 개수를 줄임으로써 데이터의 노이즈를 줄인다.
Cross-validation.
L1, L2 등 정규화.


샘플링(Sampling)과 리샘플링(Resampling)에 대해 설명해주세요. 리샘플링은 무슨 장점이 있을까요?
샘플링은 모집단에서 표본을 추출하는 것으로, 모집단을 향한 추측을 할 수 있도록 함.
리샘플링은 추출한 표본에서 다시 샘플링을 여러 번 시도하여 통계량의 변화를 확인하는 것. 종류로는 cross validation의 k-fold와 bootstrapping이 있다.
리샘플링의 장점은 정규성 등 샘플의 가정이 필요 없다는 것이며, 정확성이 향상된다.
Bootstrapping: 샘플링한 m개의 데이터 D가 있다면, D에서 데이터를 random하게 하나씩 골라 똑같은 m개의 새로운 데이터 D’를 만든다. 이 때, 추출하며 원 샘플 D에 같은 데이터를 replace한다.
K-fold와 bootstrapping의 차이점은 k-fold는 replacing 없이 추출을 하고, bootstrapping은  replacing을 하며 추출을 하는 것이다. 또한 cross validation은 모델 성능 예측에 강점을 보이며, bootstrapping은 분포 등 불확실한 통계 관련 parameter를 확인하는 데 사용된다.


일반적인 모델이 나을까요, 정확한 모델이 나을까요?
물론 task에 따라 다르다. 결국은 일반화가 어느 정도 가능하며, 동시에 데이터의 패턴을 찾아낼 수 있어야 한다. Bias-variance trade-off에서 좋은 균형을 찾을 수 있어야 한다. (좋은 균형을 찾아내려면 앙상블 모델이 주효할 때가 많다.)
복잡한 모델이 간결한 모델에 비해 유의미한 성능 향상이 없을 경우, 후자를 택한다.


feature vector란 무엇일까요?
양적 feature들을 나타내는 n차원의 벡터. Feature space는 feature vector의 vector space를 말한다. Feature 변수가 N개라면 feature space는 N차원이 된다.


Metric이란? Model accuracy를 측정할 때 가장 많이 사용하는 metric은?
모델의 성능 평가를 할 때 사용되는 지표다. 하려는 모델링이 회귀 문제인지, 분류 문제인지를 알아야 하며, label의 분포는 어떻게 되는지, task의 종류와 목표는 어떻게 되는지 등을 모두 고려해 주 metric을 결정해야 한다.


회귀와 분류에 대해 설명해주세요. 회귀 / 분류시 알맞은 metric은 무엇일까요?
회귀는 어떠한 값을 예측할 때 주로 사용하며, 분류는 데이터가 각각 어떠한 군(group)에 속하는지 판별할 때 주로 사용한다. 즉, 종속변수가 양적 데이터이면 회귀를, 범주형 데이터이면 분류를 사용한다. 분류와 회귀 모두 지도학습이다.
회귀 시 주로 사용하는 metric은 다음과 같다: MAE, RMSE, 결정계수(R^2)
분류 시 주로 사용하는 metric은 다음과 같다: Accuracy, Recall, Precision, Specificity, F1 Score, ROC/AUC, log loss


알고 있는 metric에 대해 설명해주세요(ex. RMSE, MAE, recall, precision …)
MAE: error에 절대값을 적용해 평균. 직관적이나 단위에 의존적이고 error가 overestimate인지 underestimate인지 파악할 수 없음. 이상치 handling을 잘 하지 못함.
RMSE: error를 제곱하고 합한 후 제곱근. MAE와 비슷하나 이상치에 더 민감. 즉 큰 이상치에 더 많은 페널티를 준다. MSE의 오차 왜곡 현상을 줄여주고 실제 예측값과 유사한 단위 유지. 미분 가능.
결정계수(R^2): 예측 모델에 의해 설명되는 종속 변수의 비율. 분산을 기반으로 예측 성능을 평가. 1에 가까울 수록 설명력이 높고 data에 fit하나, 우연한 상관성이 있거나 독립 변수가 많아질수록 증가해 단순 지표로만 사용하기에는 위험.
정확도(Accuracy): 정확히 예측한 값의 비율. ((TP+TN)/전체) 불균형한 분포에 취약해 좋지 않다.
Precision: 참으로 판별한 것들 중 실제 참의 비율. (TP/(TP+FP)) Recall과는 trade-off 관계.
Recall: 실제 참인 것들 중 참으로 판별한 비율. (TP/(TP+FN)) Precision과는 trade-off 관계.
F1-score: Precision과 Recall 모두 불균형한 분포에 민감하기 때문에, 그 둘의 역수를 취해 결합한 새로운 지표.
Specificity: 실제 거짓인 것들 중 거짓으로 판별한 비율. (TN/(TN+FP))
ROC curve: Recall(y) vs. 1-Specificity(x). 이진 분류의 성능을 판별할 때 쓰이며, 불균형한 분포에 민감하지 않다.
AUC: ROC curve 하의 면적. 1에 가까울수록 좋은 성능이다.
log loss: 확률적으로 잘못 분류를 했을 때 페널티를 증가시키는 방식. 확률 값을 지표로 사용하여 정답 확률이 낮아질수록 log loss 값은 기하급수적으로 증가. (예: 100% 확률/확신으로 구한 답의 log loss는 -log(1.0) = 0, 80%는 -log(0.8) = 0.22, 60%는 -log(0.6) = 0.51…) 모델 전체의 log loss를 구하려면, 계수간 log loss를 전부 더해 평균을 낸다.


왜 MSE는 좋지 않은 metric일까요? 대신 무엇을 쓰는 게 좋을까요?
큰 오차 (이상치)에 민감하고 단위가 다르기에 직관적이지도 않다. 주로 보다 직관적이며 장점은 계승한 RMSE를 사용한다.


Confusion Matrix는 무엇인가요?
분류 모델을 평가할 때 사용되는 지표인 Precision, Recall 등을 계산할 TP, TN, FP, FN을 표현한 행렬.


False positive와 false negative가 무엇인지, 왜 중요한지 설명하시고, 언제 각각의 error가 다른 error보다 중요한지 case를 들어 설명해주세요.
Confusion Matrix에 포함된 지표. FP는 실제로 음성이나 양성으로 잘못 판정된 수, FN은 실제로 양성이나 음성으로 잘못 판정된 수. 분류 모델 성능 지표에 사용되는 Precision, Recall, F1 score, Specificity, ROC/AUC에 사용되기에 매우 중요하다.
치명도가 낮으나 치료 과정이 고된 경우 (HIV), 또는 제한된 재정으로 광고나 물품을 보내는 경우, FP를 낮추는 것이 중요하다.
빠른 발견이 필요한 질병의 경우, 불량품을 판별하는 경우, 또는 물품의 출고가 그리 비싸지 않고 보다 많은 고객을 확보하는 데 중점을 둘 경우, FN을 낮추는 것이 중요하다.


Type 1 Error와 Type 2 Error는 무엇입니까?
Type 1 Error: H0을 reject해서는 안 되나, reject했을 때 (FP)
Type 2 Error: H0을 reject해야 하나, 그러지 못했을 때 (FN)


정규화(Regularization)를 왜 해야할까요? 정규화의 방법은 무엇이 있나요?
정규화는 모델의 일반화를 위해 필요하며, 모델의 복잡도를 낮춘다. 따라서 Overfitting을 방지하는 데 효과적이다. (차수가 높아질수록, 즉 모델의 복잡도가 높아질수록 overfitting되며 variance가 올라간다. 즉, 정규화를 시킴으로서 복잡도를 낮추고 variance를 낮춘다.)
정규화는 회귀 계수 β값들에 제약을 줌으로써 모델의 복잡도를 조절하며 variance 값을 조정한다. (bias는 조금 생겨날지언정)
정규화의 방법에는 L1, L2, ElasticNet이 있다.


L1, L2 정규화에 대해 설명해주세요. 장단점은?
L2 (Ridge): 계수의 제곱을 활용한다. 회귀계수의 제곱의 합을 특정 값 λ와 곱한다. λ값이 낮을수록 β값은 높아지며, 종국에는 복잡한 MSE 식과 같아진다. (즉 overfitting된다.) λ값이 높아질수록 β값은 낮아지며, 모델은 간단해지고 variance가 줄어든다. (물론 너무 줄일 경우 underfitting되며 bias가 과도하게 높아진다.)
L1 (Lasso): 계수의 절대값을 활용한다. 회귀계수의 절대값의 합을 특정 값 λ와 곱한다. λ값의 변화가 β값에 미치는 영향은 L2 정규화와 비슷하다. L1 정규화는 β값에 영향을 주는 것 이외에도 예측에 불필요한 회귀계수들을 모두 0으로 처리하며, 유의미한 회귀계수들만 선택하는 역할도 한다. 꽤 robust한 모델이기에, 다른 training set을 고르더라도 계수 선택에는 큰 변화가 없다.
(t값 역시 λ값과 비슷하다. 회귀계수의 제곱의 합을 특정 값 t보다 적도록 제한한다. t값이 낮을수록 β값도 낮아진다.)
L2는 변수 선택이 불가능한 대신 미분으로 closed form solution을 구할 수 있으며, 변수 간 상관관계가 높을 때도 예측 성능이 좋다. L1보다 조금 빠르다.
L1은 변수 선택이 가능하나 미분이 불가능하며, 변수 간 상관관계가 높다면 예측력이 떨어진다. L2보다 조금 느리다.
ElasticNet: L1과 L2를 결합한 방식. L1 정규화에 L2 제한을 가하여, 1값과 2값을 조절해 가장 예측력이 좋은 값으로 설정한다.


데이터에 노이즈를 넣어 sensitivity를 계산하는 방식은 어떻게 생각하세요?
정규화와 비슷한 역할을 할 것으로 보인다. Overfitting을 줄이고 robustness를 향상시킬 것 같다.


데이터 스케일링(scaling)이란 무엇인가요?
개별 피처의 크기를 모두 같은 단위로 변경하는 것. 피처 스케일의 차이가 클 경우, 단위가 큰 피처가 더 중요하게 연산될 수 있기 때문에 필요하다. 크게 표준화(standardization)와 정규화(normalization)로 나눌 수 있다.
표준화: 피처를 정규 분포(평균 0, 분산 1)로 변환
정규화(normalization): 값들을 전부 [0, 1] 이내로 변환
Min-max scaling: 최소값을 0, 최대값을 1로 놓고 스케일링


불균형한 데이터를 어떻게 사용할 건가요?
언더샘플링 (undersampling): 더 큰 분포의 데이터를 줄이는 것
계산 시간이 감소하지만 정보 손실.
예) random undersampling, tomek links, condensed NN, OSS (tomek links + CNN)
오버샘플링 (oversampling): 더 작은 분포의 데이터를 늘리는 것
정보 손실 없고 정확도가 보다 높지만 overfitting 가능성이 있고 노이즈에 민감하며 계산 시간 증가.
예) resampling, SMOTE, Borderline-SMOTE, ADASYN


선형 모델 (선형 회귀) 분석에 필요한 가정은 무엇인가요? 이 가정이 어긋난다면?
선형성: 독립변수와 종속변수는 선형 관계를 가지고 있음.
독립성: 독립변수 간의 상관관계가 없고 독립적.
등분산성: 분산이 같음, 즉 특정 패턴 없이 고르게 분포되어 있음.
정규성: 정규분포를 띄고 있음.
가정이 어긋난다면, 유의한 변수가 유의하지 않거나 그 반대의 경우가 나타날 수 있다.


선형 회귀, 즉 최소제곱법(OLS regression)의 공식은 무엇인가요?
MSE를 최소화하는 회귀계수 β 계산: LS=XTX-1XTy


선형 모델의 단점은? 정규화를 예로 들어 설명해 보세요.
모델이 선형이라고 가정하며, 연속적 값에만 적용 가능하다. 독립 변수가 증가할수록 분산도 증가하기 때문에 overfitting이 발생하게 되며, 이를 막기 위해 L2 정규화를 사용해 복잡도를 제어할 수 있다.


로지스틱 회귀는 무엇인가요? 스팸 필터에 로지스틱 회귀를 많이 사용하는 이유는 무엇일까요?
로지스틱 회귀는 각 속성들의 계수 log-odds를 구한 후 시그모이드 함수를 적용해 데이터가 특정 범주에 속할 확률을 [0, 1] 범주 내 값으로 예측하고 해당 확률에 따라 분류해주는 지도학습 알고리즘이다.
손실 함수(loss function)는 모델의 예측력을 확인하는 지표다. 로지스틱 회귀의 손실 함수는 log loss다.
스팸 필터는 분류 문제다. 입력값에 무관하게 일정 범위 내에서 확률을 예측하기에 해당 문제에 적절하다.


SVM이란? SVM은 왜 차원을 확장시키는 방식으로 동작할까요? 거기서 어떤 장점이 발생했나요? SVM의 장단점은 무엇인가요?
SVM이란 초평면을 활용해 데이터를 분류하는 방식으로, 선형적인 분리가 어려운 상황에서 차원을 확장시켜 면으로 분리하며 더 잘 분류할 수 있음. 군집 사이에 초평면으로 분류를 시행하며 경계에 support vector를 두고 사이 margin을 최대화시키는 방식으로 분류.
장점: 선형, 비선형 문제에 모두 사용 가능. 분류, 회귀 문제에 모두 사용 가능. 수치 및 범주 예측에 효과적. overfitting을 방지하고 오류 데이터에 민감하지 않음. 
단점: 연산이 느리고 메모리 할당량 높음. 여러 개의 hyperparameter 조합 테스트를 필요로 함.


SVM을 fit 하기 전에 차원 축소를 하는 것이 좋은가요? 이유는 무엇인가요?
때에 따라 다르다. 피처 수가 관측 데이터 수에 비해 더 많다면 (차원의 저주), 차원 축소 후 SVM을 fit하는 것이 더 좋을 것이다.


커널 기법(kernel trick)에 대해 설명해 보세요.
커널 함수를 사용하여 저차원 공간을 고차원 공간으로 매핑해주는 작업 (비선형 SVM 때 마진을 많이 남기는 초평면을 찾기 위해 사용)
커널 함수에는 다항식 커널, 가우시안 커널(GBF) 등 다양하며, 특히 GBF의 경우 모든 차수의 모든 다항식을 고려. 두 hyperparameter ɣ와 C에 의해 결정되며, ɣ는 모델의 복잡도에, C는 데이터가 모델에 미치는 영향력에 비례.
피처의 수가 많다면 SVM에 선형 커널을, 피처의 수가 적고 데이터의 수가 너무 방대하지는 않다면 GBF를 사용하곤 한다.


꽤 오래된 방법인 나이브 베이즈가 좋은 모델이 아닌 이유는? 반대로 장점은 무엇일까요? 스팸메일 모델에 어떻게 나이브 베이즈를 써서 더 발전시킬 수 있을까요?
나이브 베이즈: 모든 피처들이 독립적이고 상관성이 없다고 가정. 데이터의 정보와 클래스의 사전 정보를 결합해 베이즈 정리를 이용하여 데이터를 분류하는 알고리즘.
장점: 연산 속도가 매우 빠르다. 노이즈나 결측 데이터가 있어도 수행할 수 있다.
단점: 실생활에서 피처들이 이 가정에 들어맞는 경우는 거의 없다.
공분산 행렬을 단위 행렬로 변환해 피처 간 상관관계를 없애는 방식으로 발전시킬 수 있다.


앙상블 방법엔 어떤 것들이 있나요?
보팅: 동일 샘플에 다른 모델들을 각자 적용시켜 각자 모델 결과값의 다수결(hard voting)이나 높은 확률(soft voting) 순으로 예측.
배깅: 동일 샘플을 여러 번 bootstrapping해서 같은 얕은 모델로 여러 번 학습한 후, 보팅(질적)이나 평균(양적)으로 결과 집계. 데이터가 적어도 underfitting이나 overfitting에서 비교적 자유로움. (RF)
부스팅: 이전 모델의 오답에 가중치를 부여해 다음 모델에 적용. 배깅에 비해 정확도가 높으나, 오답을 반복적으로 학습하기 때문에 overfitting의 위험성이 높고 이상치에 취약하며 속도 느림. (GBM)
스태킹: 여러 모델이 예측한 결과값을 다시 학습 데이터셋으로 사용해서 모델 만들기. Overfitting 방지를 위해 데이터를 쪼개서 일부만 학습한 후, 각 모델의 결과들을 취합헤 meta train set을 만들어 재학습.


Decision tree는 무엇인가요? 장단점은? Pruning하는 방식은 무엇인가요?
분류와 회귀에 둘 다 쓰이며, 데이터를 두고 classification이나 threshold의 기준을 만들어 나누어 가는 방식이다.
회귀 나무를 나누는 기준은 Information gain이 가장 높은 attribute를 루트 노드에 가장 가깝게 두도록 하는 것이다 (상위 노드의 엔트로피와 하위 노드의 엔트로피의 차). 분류 나무를 나누는 기준은 불순도 측정 지표를 최소화시키는 쪽이다.
장점: 모델의 해석이 직관적이고 쉽다. 각 피처를 개별적으로 처리해 스케일링 등 전처리 과정이 불필요.
단점: 데이터의 조그마한 변화에도 영향을 크게 받는다. Overfitting의 위험성이 있다. 평균이나 다수결에 의거하여 예측을 수행하기 때문에 타 지도학습 모델보다 예측력이 떨어진다.
Pruning이란 과도한 children node 생성으로 overfitting이 된 decision tree의 가지를 줄여줌으로써 overfitting을 방지하는 것을 의미한다.
사전 가지치기: validation set을 통해 미리 depth나 sample threshold 지정
검증 데이터를 활용한 사후 가지치기 (분리 전 오분류 개수와 분리 후 오분류 개수를 비교하고 판단; 분리 전이 더 적다면 prune)


불순도 측정 지표에 대해 알고 있는 것이 있나요?
엔트로피: 정보량. 높을수록 정보량이 많고, 확률은 낮다. 로그를 취한다. 전부 같은 분류의 데이터가 있다면 엔트로피는 0, 정확히 반반이라면 엔트로피는 1이다.
지니 계수: 정답이 아닌 다른 라벨이 선택될 확률. 전부 같은 분류의 데이터가 있다면 지니는 0, 정확히 반반이라면 지니는 1이다.
엔트로피는 로그를 사용하기 때문에 지니 계수보다 연산이 느리다. 반면 엔트로피 커브가 좀 더 완만한 기울기를 가지고 있기에, 분류 성능 자체는 엔트로피가 상대적으로 나을 수 있다.


Random Forest는 무엇이며, 장점은 무엇인가요?
Decision tree를 bagging한 앙상블 모델. 주어진 feature들을 무작위로 선택해 tree를 만들어 분석하고, 이를 여러 번 반복한다.
장점: Overfitting을 방지하는 데 효과적. Scale 변화가 불필요하며, 결측치 처리에도 강함. 분류와 회귀 모두 사용 가능.
단점: 메모리 소모. Train data를 추가해도 성능 개선이 어려움.


50개의 작은 Decision tree는 큰 Decision tree보다 괜찮을까요? 왜 그렇게 생각하나요?
50개의 작은 Decision tree를 사용하는 앙상블 기법, 즉 Random Forest는 단일 Decision tree보다 error 교정에도 강하며 overfitting의 확률도 적다. 이상치에도 과도하게 반응하지 않는, 보다 robust한 모델이다.


Gradient Boosting 회귀 트리는 무엇인가요? XGBoost는 무엇인가요?
Gradient Boosting 회귀 트리는 Decision Tree의 앙상블 모델이나, 배깅 (Random Forest) 대신 부스팅 앙상블 모델이다. Iteration 수나 learning rate (오차 보정치) 등 hyperparameter 설정에 보다 민감하지만 더 높은 정확도를 제공해 줄 수 있다. 다만 훈련 시간이 길다. 분류와 회귀 문제에 모두 사용 가능하다.
XGBoost는 일반 GB에 비해 다음과 같은 장점들을 발전시킨 패키지로, 매우 각광받고 있다.
보다 빠른 수행 시간 (병렬 처리)
정규화 규제, early stopping 등 추가 기능을 통해 overfitting 방지 가능
자체적으로 결측치 처리


Random Forest와 SVM을 각각 언제 쓸 것인지 설명해주세요.
Random Forest: 데이터의 수가 많거나 데이터의 형태(양적, 질적)가 일정하지 않을 경우 (혹은, 데이터 전처리가 되어 있지 않은 경우), 모델링을 빠른 시간 내에 완수해야 할 경우, 다중 분류를 해야 할 경우
SVM: 스케일링 되어 있고 양적 데이터 (혹은 one-hot encode된 질적 데이터)의 binary 분류 문제의 경우, 메모리 capacity 제한이 널널한 경우
SVM은 조건이 까다롭지만, 조건이 충족되는 경우에는 거리 metric이 중요하다는 가정 하에 Random Forest보다 나은 성능을 보인다.


k-NN과 K-means의 차이점에 대해 설명하세요.
k-NN: 주로 분류 (지도학습); 가끔 회귀 (지도학습)에도 사용. Target label이 존재함.
K-means: 군집화 (비지도학습). Label 없는 데이터와 threshold만이 주어짐.


K-means나 k-NN에 맨해튼 거리를 사용하지 못하는 이유는 무엇인가요?
해당 학습은 centroid를 중심으로 군집화하는 알고리즘이며, centroid는 유클리디안 거리를 사용하는 지표.

K-means에서 최적의 k값은 어떻게 구할 수 있을까요?
Elbow (inertia): SSE vs. k 그래프를 활용해 elbow의 x값 찾기
실루엣 계수: 각 군집 간 거리가 얼마나 효율적으로 분류되었는가를 나타냄. [0, 1] 내의 값으로, 1에 가까울수록 최적화가 잘 되어 있다 (즉, 더 효율적으로 분류되어 있다).

K-means의 대표적 단점은 무엇인가요?
시작 지점이 random하게 지정되기에 최적 값이 달라질 수 있다. (local minima)
k값에 따라 성능이 달라진다.
노이즈가 많은 경우 효과적이지 않다.


K-means의 단점을 보완할 만한 군집화 방법은 무엇이 있을까요?
계층 군집화 (hierarchical clustering): 병합 군집으로 각각의 데이터 포인트로부터 병합 식으로 군집화를 실행하며, dendrogram으로 군집을 시각화한다. k값을 지정할 필요가 없으며 시작 값에 영향을 받지 않는다는 장점이 있으나, 복잡한 형상의 데이터 구분에는 약하다.
DBSCAN: 가장 붐비는 지역의 데이터를 선택한 후 이를 핵심 샘플로 두고, 특정 거리 내 있는 모든 포인트를 경계 포인트로 둔 후, 해당 경계 포인트의 특정 거리 내 일정량의 데이터가 없다면 노이즈 분류, 그 외에는 핵심 포인트 분류. 복잡한 형상의 데이터 구분에도 강하고 k값을 지정할 필요가 없으나 느리다. 또한 크기가 다른 군집을 만들어낼 수 있다 (장단점).

Local Minima와 Global Minima에 대해 설명해주세요. 특히 k-means clustering에서, local minima가 왜 중요한가요? Local minima 문제가 생겼다는 걸 어떻게 알 것이며, 어떻게 대처하나요?
Local minima는 주변 값들과 대조했을 때 최적의 결과를 도출하는 값.
Global minima는 모든 값들과 대조했을 때 최적의 결과를 도출하는 값.
K-means clustering 분석을 할 때, 비용 함수는 minima에 도달할 때까지 최적화를 계속할 것이다. Minima에 다다랐을 때, 군집화가 최적화되었다고 판단한다.
최적 값을 비교적 빨리 찾았거나, 시작 값에 변화를 줄 때 최적 값이 다르게 나타난다면, local minima 문제를 겪고 있을 확률이 높다. K-means clustering 분석에서 해당 문제를 해결하려면 여러 k값을 시도하며 가장 적은 비용을 리턴하는 값을 최적 값이라고 판단한다. 다른 분석(예: GD)에서는 learning rate를 알맞게 조절한다.


차원의 저주에 대해 설명해주세요. 거리 및 유사도 지표에 어떤 식으로 영향을 미치나요?
피처의 차원이 커지면서 발생하는 문제 (피처의 수가 데이터 수보다 많아질 때). 차원이 커지면, 공간이 커지면서 차원 내 데이터도 sparse해지고, 데이터가 어디에 존재하는지 locate하기 어려움. 또한 메모리를 불필요하게 많이 차지하게 됨.
k-NN 알고리즘에 특히 치명적. 데이터 간 거리가 점점 늘어남에 따라, 거리를 기반으로 이웃 군집을 설정하는 k-NN 알고리즘의 분류 성능에 큰 타격을 입힘.
다른 알고리즘을 사용하거나 차원 축소가 필요.


차원 축소의 중요성은? 차원 축소 기법으로 보통 어떤 것들이 있나요?
차원 축소는 메모리 최적화, 다중공선성 제거, 시각화 발전, 차원의 저주 방지에 큰 도움이 된다.
차원 축소는 피처 선택과 피처 추출로 나눌 수 있음. 피처 선택이란 다중공선성 등 불필요한 피처를 제거하는 것이며, 피처 추출이란 기존 피처를 저차원의 중요 피처로 압축해서 추출하는 것.
피처 선택의 장점은 선택된 피처의 해석이 용이하다는 것. 단점은 피처간 상관관계 고려 어려움. 피처 추출의 장단점은 피처 선택의 장단점의 정반대.
피처 선택 기법: filtering (통계 분석으로 최적의 feature subset 선택; t-test, chi-squared test 등), wrapper (feature subset을 계속 만들어 반복적으로 test; forward greedy, backward greedy 등)
피처 추출 기법: PCA, SVD, NMF, LDA


PCA, LDA, SVD 등의 약자들이 어떤 뜻이고 서로 어떤 관계를 가지는지 설명할 수 있나요? PCA와 SVD의 관계성에 대해 설명해주세요.
PCA(Principal Component Analysis)는 데이터의 공분산 행렬을 기반으로 고유벡터를 생성하고 이렇게 구한 고유 벡터에 입력 데이터를 선형 변환하여 차원을 축소하는 방법이다.
SVD(Singular Value Decomposition)는 PCA와 유사한 행렬 분해 기법을 사용하나 정방 행렬(square matrix)를 분해하는 PCA와 달리 행과 열의 크기가 다른 행렬에도 적용할 수 있다. 차원 축소에도 사용하고, 행렬 재구현에 특히 뛰어나기에 이미지 처리에도 좋다. 예측하는 데에는 그리 뛰어나지 않으며, data sparsity에 취약하다.
(mn 행렬을 mm, mn, nn으로 나누어 계산한다.)
LDA(Linear Discriminant Analysis)는 지도학습에서 사용하는 차원 축소 기법 중 하나로 특정 공간상에서 군집 클래스를 명확히 분리시키는 것에 집중한다. 즉, 분류하기 쉽도록 클래스 간 분산을 최대화하고 클래스 내부의 분산은 최소화하는 방식을 말한다.


PCA는 무엇인가요? PCA를 쓸 만한 문제들은 무엇이며, 단점은 무엇인가요?
비지도 변환의 하나로, 데이터의 분산을 유지할 수 있는 (즉, 정보를 가장 덜 손실하는) vector를 찾는 분석. 차원 축소 기법 중 하나로, 분산이 최대인 축을 찾은 후, 분산을 유지할 수 있는 축을 추가로 찾는다.
공간 데이터를 많이 소요하는 문제나 여러 차원의 피처들을 시각화할 때 사용한다.
단점: 각자 다른 스케일에 영향을 받으며, 변수 간 상관관계가 없을 경우 차원 축소 대신 분산 순으로 나열밖에 하지 않는다. 또한 차원 축소를 실행했기 때문에 특성이 혼합되어 시각화된 축의 의미를 파악하기 어렵다.


PCA는 차원 축소 기법이면서, 데이터 압축 기법이기도 하고, 노이즈 제거기법이기도 합니다. 왜 그런지 설명해주실 수 있나요?
PCA는 입력 데이터의 공분산 행렬을 기반으로 eigenvector를 생성하고 입력 데이터를 선형 변환하여 차원 축소를 실행한다. 즉, 입력 데이터의 피처 역시 압축되기에 데이터 압축 기법이라고 볼 수 있다.
또한 eigenvalue가 가장 큰 (데이터의 분산이 가장 큰) 순서대로 주성분 벡터를 추출하기 때문에, 피처의 설명도를 순차적으로 나열할 수 있어 노이즈 제거 기법으로도 볼 수 있다.


PCA 이외의 차원 압축 기법을 알고 있나요? (NMF와 t-SNE는 무엇인가요?)
NMF: PCA와 유사하나, 음수가 아닌 데이터에서 성분과 계수 값을 찾는다. 성분에 음수가 없어야 한다는 제약이 있으나, 음수 값과 양수 값이 상쇄되는 PCA보다 대체적으로 직관적이다. PCA와 달리 성분 간의 서열을 두지 않고 특징을 나눌 수 있다.
t-SNE: 복잡한 매핑으로 시각화 도출에 좋은 매니폴드 학습 중 하나. 데이터 포인트 사이의 거리를 가장 잘 보존하는 2차원 표현을 찾는다. 즉, 이웃 데이터 정보를 보존하는 데 집중한다.


(다중)공선성은 무엇이며, 어떻게 해야 하나요? 어떻게 다중공선성을 없앨 수 있나요?
다중 회귀 모델을 구축할 때, 2개 이상의 피처가 매우 높은 상관관계를 가지고 있는 것을 의미. 중복된 정보를 제공할 수 있다.
다중공선성이 문제되는 이유는, 독립성이 위배되어 회귀계수 값이 불안정해지며 표준 오차가 늘어나기 때문. 즉, 각 변수의 설명력이 약해진다.
다중공선성을 점검하려면 변수 간 상관관계를 조사하는 방법이나 VIF를 조사하는 방법을 사용해야 한다. VIF란 독립변수를 하나씩 종속변수로 지정하고 나머지 독립변수로 회귀 분석을 진행하는 것으로, 큰 값이 반환될 수록 다중공선성을 증명한다.
다중공선성을 없애려면 높은 상관관계에 있는 변수를 하나 없애거나, 변수들을 하나로 취합하는 방법이 있다.


토픽 모델링이란 무엇이며, LSA(Latent Semantic Analysis)와 LDA(Latent Dirichlet Allocation)란 무엇인가요? 언제 쓰이며, 단점은 무엇인가요?
토픽 모델링: NLP에서 토픽이라는 문서의 주제를 발견하기 위한 통계적 모델. 텍스트 마이닝 기법으로, 텍스트의 숨겨진 의미 구조를 발견하는 데 중점을 둠.
Truncated SVD는 SVD와 똑같으나 상위 n개의 특이값만 사용하는 축소 방법이다. 이 방법을 쓸 경우 원 행렬로 복원할 수 없다.
LSA는 잠재 의미 분석을 말하며, 주로 토픽 모델링에 자주 사용되는 기법이다. LSA는 DTM이나 TF-IDF 행렬에 Truncated SVD를 적용하여 차원을 축소시키고, 단어들의 잠재적인 의미를 이끌어낸다. TF-IDF 기법에 비해 단어의 의미를 신경쓰며 연산이 빠르다는 장점이 있으나, 새로운 문서가 추가되면 연산을 다시 해야 한다는 단점이 있다. 또한 단어 순서는 신경쓰지 않는다.
LDA는 토픽 개수 n개를 정하고 모든 문장들의 단어를 n개의 토픽 중 하나로 분류한 후, 잘못 분류된 토픽은 다른 모든 토픽이 제대로 분류되었다고 가정하고 해당 문장이나 전체 문서에서 토픽별 분포를 기반해 다수 쪽으로 결정하는 토픽 모델링이다. 단어 순서는 신경쓰지 않는다. 피처의 수를 줄일 때 사용하기 좋으며, training set이 작을 때 유용하다.


텍스트 더미에서 주제를 추출해야 합니다. 어떤 방식으로 접근해 나가시겠나요?
토픽 모델링. 세부적으로는 LDA 사용.


Markov Chain은 무엇인가요?
Markov 성질이란 오직 이전 단계, 혹은 이전 n개의 단계에서의 결과값에만 영향을 받는 것을 뜻함.
Markov Chain은 이 Markov 성질을 따른 데이터에서 이산 시간 확률 과정을 바탕으로 연산하는 것.


연관규칙분석(Association Rule)의 Support, Confidence, Lift에 대해 설명해주세요.
연관규칙분석 (장바구니 분석): 일련의 거래 또는 사건들 간의 규칙을 발견하기 위해 사용.
평가 지표:
Support: 전체 거래 중 항목 A와 B를 동시에 포함하는 거래의 비율
Confidence: 항목 A를 포함한 거래 중 항목 A와 B를 동시에 포함하는 거래의 비율 (연관성의 정도 파악 가능)
Lift: A가 포함되지 않은 거래 중 B가 포함된 비율로부터, A가 포함된 거래 중 B가 포함된 비율로의 증가비 (서로 관련이 없을수록 Lift는 기본비인 1과 가까움)

머신러닝(machine)적 접근방법과 통계(statistics)적 접근방법의 둘간에 차이에 대한 견해가 있나요?
접근 방식이 다르다. 머신러닝은 예측 성능을 높이는 데 집중하기 때문에, 다양한 피처를 사용하여 높은 예측률을 달성하는 것이 목표다. 통계적 접근방법은 분포와 가정을 통해 신뢰 가능하고 정교한 모델을 만드는 것을 주 목표로 한다.


인공신경망(deep learning이전의 전통적인)이 가지는 일반적인 문제점은 무엇일까요?
딥러닝 이전의 인공신경망은 선형적 회귀나 분류 방식만을 택했기에 XOR 등 비선형 방식을 필요로 하는 문제를 해결하지 못했으나, 시그모이드 함수 등 비선형 방식과 오차역전파의 모델 업데이트 방식을 발전시켜 문제점을 극복했다.


여러분이 서버를 100대 가지고 있습니다. 이때 인공신경망보다 Random Forest를 써야 하는 이유는 뭘까요?
Random Forest는 Decision Tree의 배깅 앙상블 알고리즘이기에, 각 서버를 하나의 Decision Tree로 병렬적이게 구성 가능하나, 인공신경망은 서버 하나하나가 모델의 특성을 모두 이해하는 end-to-end 구조로 직렬적이게 구성된다. 따라서 병렬적으로 여러 서버를 활용할 수 있는 Random Forest 방식을 사용한다.


공분산(covariance)과 상관도(correlation)를 비교하세요. 
공분산은 두 개의 random 확률변수가 어떠한 상관성을 띄고 있는지 알려준다. (변수의 크기에 큰 영향을 받는다.)
상관도는 두 개의 변수가 어떠한 상관성을 띄는지, 얼마나 띄는지를 알려준다. (크기 영향 없음)


Continuous와 categorical 변수 사이의 correlation은 어떻게 계산하나요?
Categorical 변수가 binary일 때만 가능. 0과 1로 코딩하고 Pearson 상관계수를 계산 (점이연 상관계수)


PCA에 있어서 rotation이 꼭 필요할까요?
필수적이지는 않지만, 연산에 있어서 최적화를 시켜주고 해석력을 향상시켜 줌.


베이즈 정리를 머신러닝에 관련지어 설명하세요.
베이즈 정리는 관찰결과를 통해 사후 확률을 지속적으로 업데이트할 수 있다. 머신러닝의 모델 업데이트와 유사한 방식으로 작동되며, 나이브 베이즈 모델에 적용될 수도 있다.


유저가 10만명, 아이템이 100만개 있습니다. 이 경우 추천 시스템을 어떻게 구성하시겠습니까?
관련 피처를 추출 및 선택하고 차원 축소 및 모델링.
추천 시스템
추천 시스템에서 사용할 수 있는 유사도 지표는 무엇이 있을까요?
피어슨 상관계수
두 벡터 사이의 선형 상관관계를 찾음 (-1 ~ 1) for CF
특별히 어려운 계산이 없는 단순한 알고리즘 (무난무난)
모든 neighbor의 rating이 동등하게 “가치 있는” 것은 아닐 것이다. (베스트셀러는 누구나 다 좋아하지…)
가중치로 더 발전시킬 수 있을 듯. 분산이 많은 것에 가중치, 많은 데이터를 바탕으로 유사도가 높은 유저에 가중치…
코사인 유사도
IBCF에 좋음. 사용자의 평가값을 좌표로, 아이템을 하나의 차원으로 해서 두 벡터 사잇각을 구한다. (유사도가 높을수록 코사인값이 크다)
자카드 유사도
Binary 데이터일 경우; 교집합 / 합집합

추천 시스템에는 크게 두 가지 방식이 있습니다. 무엇인가요?
CB (Content-based filtering; 콘텐츠 기반 필터링)
사용자가 특정 아이템을 선호하는 경우, 그 아이템과 비슷한 콘텐츠를 가진 다른 아이템 추천
텍스트 기반 데이터라면, NLP (자연어 처리):
TF-IDF로 빈도 기반 단어 중요도를 구할 수도 있고, RNN으로 순서 고려도 가능
Word2Vec
토픽 모델링 (LDA)
이미지나 음악 기반 데이터라면, CNN (이미지, 음악), Mel-spectogram (음악), MFCC (음악)
CF (Collaborative filtering: 협업 필터링)
유사성을 사용해 취향이 비슷한 사람들(UBCF)이나 특성이 비슷한 아이템들(IBCF)을 찾아 그들의 평점 등을 비교해 이에 기반해 추천.
먼저 데이터의 종류를 관찰하고 어떤 유사도 지표를 사용할지, UBCF/IBCF 중 어떤 방식을 채용할지 결정하고 그에 따른 parameter를 변경해 가며 추천 시스템의 오류를 줄인다. 또한 유사도 지표의 parameter나 threshold도 바꿔가며 유사도 측정 방식에 차이를 둔다.
메모리 기반 CF
UBCF
kNN: 가장 유사한 k명을 이웃으로 취급
평가경향 고려 가중
사용자가 평점을 부여하는 경향을 고려해 가중치를 부여. Bias를 피하기에 좋다. 평점을 (평점-사용자 평균 평점)으로 변환한 후, 차이의 예측값을 구해 현 사용자의 평균에 더해준다.
신뢰도 가중
신뢰도 (공통 평가 아이템 수)가 일정 이상인 유저만 취급하고 신뢰도 순으로 유사도에 가중치 부여. RMSE의 변동성이 높기에 selective하다.
간단하고 직관적이며 데이터 추가에도 안정적이나, 느리고 sparsity 때문에 범위가 제한적이다 (예: 유저가 평가한 아이템이 다른 유저에게 외면당하면?)
Thresholding: 유사도를 일정량으로 정하고 통과하는 유저를 이웃으로 취급. 통과하는 유저가 적기 때문에 kNN보다는 덜 사용된다.
IBCF: 사용자가 평가한 아이템의 평점과 해당 아이템과 다른 아이템의 유사도를 가중해 평균한 값을 예측값으로 사용
모델 기반 CF
Matrix Factorization (MF)
SVD의 응용. SVD처럼 세 개의 행렬로 나누는 대신, null 수치가 많은 평점 행렬 R를 사용자 잠재 행렬 P와 아이템 잠재 행렬 Q로 분할해 계산한다. PQ=R'를 구축한 후, R’를 R에 최대한 유사하게 만드는 것이 목표다. ([mn]=[mk][kn])
Latent 수 k를 정하고 P, Q 행렬을 임의로 채운 후, 예측 평점을 받아 오차를 줄이는 목표로 P, Q 값을 수정한다. 평가 메트릭인 RMSE가 기준 이하거나 iteration threshold에 도달하면 정지. (RMSE를 통해 k값과 iteration의 최적치를 찾는다)
장점: sparsity에 강함, 간단한 feature engineering, 간단한 scaling
단점: cold-start에 약함, metadata에 경우 바로 적용할 수 없음 (rating 형태가 아니라서?)


CB와 CF의 단점은?
CB: 유저의 interest 내에서만 추천이 돌기 때문에 interest 확장 불가, embedding의 경우 metadata의 질에 따라서 모델이 좌우됨
CF: Cold-start, First-rater 문제에 취약. Popularity bias. 확장성을 고려해야 함.


UBCF와 IBCF의 장단점을 설명하세요.
UBCF: 데이터가 풍부할 경우 정확도 높음, 사전 질문이 있을 경우 cold-start에서 그나마 자유로움. 표본이 적을 경우, 뜬금없는 추천을 하기도 함. 데이터가 변화할 때마다 업데이트해야 함.
IBCF: 사용자별 계산이 아니기에 계산이 빠르고 수시 업데이트가 필요하지 않음. 단기간에 고효율, 안정성도 있음. UBCF보다는 정확도가 떨어지고, cold-start에 매우 취약.


MF의 방식을 나열해 보세요.
SGD (Stochastic Gradient Descent)
Loss function인 RMSE를 최소화하기 위해 MF의 P, Q 두 행렬을 동시에 최적화. 편미분을 통해서 권장 p, q값을 업데이트하고, 여러 iteration을 통해 loss function을 최소화한다. Overfitting 방지를 위해 L2 정규화 사용. 또한 전체평균을 제거하고 사용자의 평가경향을 수정해야 한다. (즉 p, q, bi, bj 동시 수정)
장점: 유연하며, 여러 loss function에 적용 가능
단점: 느린 속도, 미관측 항목은 다루기 어려움
ALS (Alternating Least Squares)
목적함수를 최적화하는 기법으로, 사용자와 아이템의 Latent Factor를 한 번씩 번갈아가며 학습. 아이템의 행렬을 상수로 놓고 사용자 행렬을 학습시키고, 사용자 행렬을 상수로 놓고 아이템 행렬을 학습시키는 과정을 반복함으로써 최적의 Latent Factor를 학습시키는 방법.
SGD는 두 행렬을 동시에 최적화. ALS는 한 번씩 고정시키고 나머지 하나를 최적화.
장점: 보다 빠른 속도, 미관측 항목도 다루기 쉬움
단점: loss square에만 의존
NMF (Non-negative MF)
피처들이 모두 양수일 때, MF 진행. 데이터들이 항상 직교성을 띄지는 않기에 피처 간 독립성을 더 잘 볼 수 있다.
Low-rank MF: Truncated SVD처럼 차원을 낮추어서 사용.
BPR-MF: 사용자가 상품에 대한 implicit feedback을 제공했는지 유무만 고려해 사용.


딥러닝을 어떻게 추천 시스템에 활용할 수 있을까요?
구현 (Layer를 쌓는 방식)
사용자와 아이템을 one-hot representation으로 변환 (input layer)
각 사용자/아이템 구분 위해 NxN, MxM 정사각형 행렬 (0/1)
각각의 layer를 잠재요인(embedding layer)으로 연결 (Nxk, Mxk 개의 연결)
Input layer에서 user bias embedding, item bias embedding 추출 (동 노드 수)
(결과물: user latent vector, item latent vector, user bias embedding, item bias embedding [layer 1] – concatenate)
Layer가 거듭될 수록 bias 하나 추출하고 뒷 layer와 dot product 연결
Local minima 문제나 vanishing gradient 문제를 피하기 위해 learning rate를 적절한 값으로 설정하고, mini-batch로 쪼개어 SGD를 실행하는 mini-batch GD를 실행한다.


성능 평가를 위해 어떤 지표를 사용할까요?
예측 metric: 예측한 값이 정확한지? (회귀 등 연속형 변수)
MAE, RMSE
추천 metric: 추천한 item이 제대로 된 추천인지 아닌지?
Accuracy, Precision, Recall, F1-score, ROC/AUC (분류 등 이산형 변수)
랭킹 metric (Learn To Rank): 추천하는 순위를 중점으로 성능을 평가
Pointwise
손실 함수에서 한 번에 하나의 아이템만을 고려하는 방법으로, 하나의 사용자에 대응하는 하나의 아이템만을 가지고 Score를 계산하고 이를 Label Score과 비교해서 최적화.
장점: 직관적이라 기존 분류/회귀 모델 사용 가능
단점: 아이템 간의 순서 관계를 무시하고 독립적인 개체로써 학습시키고 결과만을 정렬
Pairwise
손실 함수에서 한 번에 2개의 아이템을 고려하는 방법. 1개의 긍정적 아이템과 1개의 부정적 아이템을 고려하기 때문에, 데이터 셋에 {사용자, 긍정적 아이템, 부정적 아이템}의 3가지 항목이 포함되어야 하고, 이로 인한 데이터 중복을 해결하기 위해 Sampling 기법이 활용됨.
BPR (Bayesian Personalized Ranking): 아이템에 대한 사용자의 선호도를 확률 모형화한 모델로, 사용자가 선호하는 아이템을 단계별로 카테고리화(긍정적 아이템, 부정적 아이템) 해서 분석을 진행함. 아이템을 카테고리화할 때 사용자가 내재적 피드백을 제공했는지 안 했는지의 정보만을 이용하기 때문에 정보의 손실이 발생할 수 있지만, 기존의 기법대비 우수한 성능을 보이는 모델.
Listwise
손실 함수에서 한 번에 모든 아이템을 고려하는 방법. 전체 item의 rank 고려.
MRR (Mean Reciprocal Rank)
사용자별로 관련 있는 상품 중 가장 상위에 위치한 rank를 기록, 1최상위 rank를 사용자별로 모두 더한 평균값.
장점: 매우 쉽고, 최상위 1개의 컨텐츠에만 관심 있을 경우 유용. (배달 앱의 특정 음식점 추천 등)
단점: 탐색 컨텐츠가 2개 이상일 경우 매우 비효율적. 이산형만 가능.
MAP@K (Mean Average Precision @ K)
상위 K개의 추천 아이템 중, 사용자가 선택한 평균 비율.
(즉, 만약 MAP@3이라면 모든 사용자에게 AP@3을 실시해 평균을 낸다.)
추천 순서에도 영향을 받는다. 상위 3개 중 추천 item이 선택되었는지 여부의 행렬이 [0, 1, 0]이라면, AP@3은 (1/3)(0/1+1/2+1/3)으로 계산한다.
장점: 단순한 성능이 아니라, 우선순위의 성능까지 계산.
단점: 클릭과 같은 이산형 데이터가 아니라 평점 데이터라면 관련 여부 판별 어려움. (평점 3은 관련 있는 지표일까 아닐까?)
nDCG(@ K)
가장 이상적인 추천 조합 대비 현재 모델의 추천 리스트가 얼마나 좋은지 나타내는 지표. 랭킹에 민감한 추천서비스의 경우 유용.
실제 랭킹2이상 랭킹으로 DCG를 계산하며, 이를 최대 DCG 값, 즉 IDCG로 나눈다. IDCG는 분자가 이상 랭킹의 반대 순으로 기록된다.
nDCG는 0~1사이의 값을 갖게되는데 1과 가까울수록 우수한 추천 시스템.
장점: 연속형, 이산형 변수에 모두 사용 가능, 작은 스케일을 가지기에 비교에 더 용이. K값에 어느 정도 독립적이어서 K의 최적화를 객관적으로 실행 가능.
단점: bias에 취약, sparsity에 취약
비즈니스 metric: 비즈니스적인 관점에서 추천 시스템의 성능을 어떻게 평가하는지?
CTR (click-through rate): 사용자의 클릭율. 클릭 수노출 수100. 대표적인 평가 지표.
그러나 추천이 잘 되어도 CTR이 감소할 수 있다.
Duration rate (체류 시간), Retention rate (재방문율)
Hit rate: 사용자별 선호 아이템에서 1개씩을 제외한 후 학습, 추천했을 때 K개 중 선호 아이템이 있다면 hit, 없다면 miss
품질 관점 metric:
Diversity: 각 사용자에게 얼마나 다른 상품들이 추천되었는가?
Intra-list Similarity: 제품들 간의 (코사인 등) 유사도를 전부 계산해 더하고 평균 구하기. Pairwise하게 유사도를 구하기 때문에 제품의 수가 많을수록 계산이 오래 걸린다. 높다면 추천된 제품들 간의 유사도가 높다는 뜻이며, 목표가 무엇이냐에 따라 지표를 다르게 해석한다.
Coverage: 전체 아이템 중 추천 시스템이 추천하는 아이템의 비율 (CF는 낮은 coverage를 지니고 있어 문제가 된다. 다시 말해, 1000개의 item 중 편향 때문에 100개의 item밖에 추천하지 못한다면, 10%의 낮은 coverage를 지니게 되고 나머지 900개는 추천하지 않게 된다.)
Personalization: 서로 얼마나 다른 제품 리스트를 추천해주는가?
유저들의 rating history를 고려해 유저 간 코사인 유사도 행렬을 만들고, upper triangle 부분의 평균값을 구한다. 이를 1에서 빼주면 personalization. 높다면 유저들 간 추천 제품들이 서로 다름을 의미한다.
Serendipity (우연성): 보편적인 제품들 말고, 얼마나 새로운 상품이 들어왔는가?
유저들의 흥미 감소를 방지, long-tailed 분포 문제 방지. 그러나 우연성을 과도하게 강조하면 안됨. 헤비유저들에게 좀 더 높이, 라이트 유저에게는 연관성을 더 높이기.


Retention rate는 실제로 어떻게 평가받아야 하나요?
높은 retention rate는 현 고객들이 계속해서 제품 구매나 사이트 방문을 한다는 지표이며, 반대로 낮은 retention rate는 churning이 지속적으로 일어나고 있다는 것을 시사한다.
시간 기반 retention rate는 주로 N-day 방법으로 계산한다. Active user와 time period를 정의해야 retention rate를 계산할 수 있다.
Retention rate의 감소를 방지하려면 at-risk 단계에 있는 고객들의 churning 시기 이전에 그들의 피드백을 수용하거나 구매 history를 사용해 UX를 향상하는 등 그들에게 어필해야 한다.
장기 고객과 신규 고객의 retention rate에는 차이가 있을 듯 하다. 이러한 trend 차이가 언제 일어나는지 파악하고, 해당 기간을 신규-장기 전환 시기로 지정한 후, 신규 고객의 retention rate에 보다 집중하는 것이 좋을 듯 하다.


Explicit Feedback과 Implicit Feedback은 무엇일까요? Implicit Feedback을 어떻게 Explicit하게 바꿀 수 있을까요?
Explicit Feedback: 유저로부터 선호도를 직접적으로 수집하여 반영하는 방법.(좋아요, 리뷰, 구독, 차단 등) 유저의 의사가 적극적으로 반영된 데이터이므로 높은 신뢰도와 강력한 영향력을 가지지만, 수집 난이도가 높다.
Implicit Feedback: 유저의 행동으로부터 선호도를 간접적으로 수집하여 반영하는 방법. (검색 기록, 방문 페이지, 페이지 체류 기간, 구매내역, 클릭 등) 수집 난이도가 낮아 동원할 수 있는 데이터의 양은 많지만, 데이터에 노이즈가 많고 분석 방법에 따라 해석 결과의 차이가 생길 수 있다. 부정적인 피드백이 없다. 노이즈가 많으나, 높은 값은 신뢰도를 보장한다고 해석해도 괜찮을 듯 하다.
Implicit을 explicit하게 바꾸려면, item의 availability나 반복되는 feedback 등을 고려.
availability: 만약 두 가지 상품 중 1택을 해야 한다면?
반복 feedback: 1회성 방문과는 어떻게 다르게 해석할까?


Cold Start란? 이 경우엔 어떻게 추천해줘야 할까요?
새로운 서버, 새로운 아이템(cold-start), 새로운 유저(first-rater) 등 정보가 없는 경우에 발생하는 추천 시스템 문제. Cold-start의 경우 아이템 정보가 없기 때문에 CF에는 큰 타격이며, CB는 상대적으로 덜하나 만약 컨텐츠 지표에 유저 관련 피처가 있다면 CB도 타격. First-rater의 경우 IBCF에는 큰 타격이며, 사전 조사를 했을 경우 UBCF는 그나마 추천해 줄 거리가 생긴다.
해결법:
베스트셀러/인기제품 추천
UX 편향 없는 인기제품 추천 (최근 컨텐츠는 특히 UX 편향이 심함)
새 아이템 hybrid modeling (글/기사의 경우): 30분 내 CB, 반응이 일정량 모이면 CF, 실시간 MAB 업데이트
유저 초기 프로필 완성 (Preference Elicitation): 신규 아이템 프로필 완성 (유사한 아이템 평점의 평균… 권장 X)
ML - Active Learning: 유저에게 다수의 유사 그룹에서 각각 하나의 제품을 골라준 뒤 평가하게 함으로써 그것을 기반으로 추천.
Hybrid Feature Weighting: 단순한 CB가 아니라 컨텐츠 내 특성 - 배우 등 - 까지 고려해 하나의 아이템으로부터 추천
Regularization 가중치에 차이를 둬 모델을 일반화 시키기: common sense와는 반대로 활성화된 유저나 베스트셀러에 정보 조금, 활동 안하는 유저나 인기없는 제품에 정보 많이. overfitting 방지하듯이.
유사 이웃을 잘 정해 UBCF를 잘 할 수 있도록 유저의 social 정보를 취합 (새 데이터가 필요하다는 약점)
여러 Interface Agents: 다수의 implicit 행동 패턴을 취합한 후 그에 따라 추천.


A/B 테스트란 무엇인가요? A/B 테스트의 장점과 단점, 그리고 단점의 경우 이를 해결하기 위한 방안에는 어떤 것이 있나요?
동일 집단을 임의의 두 집단으로 나누어 각기 다른 모델로 테스트. (보통 새 모델은 5-10%에만)
장점: 다른 변수들을 통제 가능하기 때문에 직관적으로 두 모델의 차이를 알 수 있음, 적은 비용.
단점: 주기적으로 실행할수록 효과 감소, 실험 기간이 길수록 취향의 변화 등 시간적인 요인 때문에 결과에 영향이 갈 수 있음, 작은 변화만 주기 때문에 local minima 문제에 빠질 수 있음.
해당 단점들을 타파하기 위해 MAB를 사용할 수 있다.


만약 A/B 테스트에서 기존 모델에 비해 새 모델의 metric이 매우 적은 차이로 더 나았고, p-value는 매우 적은 값이 나왔다면 모델을 바꾸는 것이 좋을까요?
A/B 테스트의 샘플이 대표성을 띠었는지 확인해야 함. A/B 테스트의 기간을 확인하고, 데이터가 계절이나 달의 영향을 받을 수 있는지, 트렌드의 반영에 민감한지 등을 확인. 2022년 데이터가 2023년 데이터와 동일하게 반응할지 아무도 모르기에 실험 기간은 중요. Sample size 또한 확인할 필요가 있다.
가장 중요한 것은 모델을 바꾸는 데 드는 비용. (이익비용)의 기간이 터무니없이 길다면 모델 변경이 도움되지 않을 것.


MAB란 무엇인가요?
탐색과 활용을 바탕으로, 가장 수익률이 높은 슬롯머신을 찾아내 수익을 최대화하는 방식. 탐색을 너무 많이 하면 탐색 비용이 많이 들어가고, 탐색을 너무 적게 하면 최적 수익률 슬롯머신을 잘못 판단하므로 탐색과 활용의 적절한 균형이 필요하다.
실생활에서는 상당히 까다롭다. 하나가 아니라 여러 슬롯머신을 시도해야 하고 승률도 계속 변하기 때문. 또 시간, 위치 따라 승률도 다르다. 새 슬롯머신 추가 때 초기화 방법도 쉽지 않다. 선택 후 승률 도출도 어느 정도의 딜레이가 있다.

MAB를 실행할 수 있는 방법들은 무엇인가요? (Greedy, Epsilon-greedy algorithm 제외)
UCB (Upper Confidence Bound): 기본적으로는 임의(0.1-0.3)의 ε값을 활용해 확률적으로 greedy나 랜덤 슬롯머신 활용을 실행하는 epsilon-greedy algorithm과 같으나, 플레이한 횟수가 적을 경우 수익률이 최적이 아니더라도 최적일 가능성이 있다고 가정해 가중치를 더해준다. 즉, 수익률 또한 일정량 고려하지만 플레이한 횟수가 적을수록 다음에 선택할 확률을 조금 더 높인다.
Thompson Sampling: 광고 추천에 많이 적용. 방식은 후술.

Thompson Sampling은 어떻게 작용하나요?
MAB의 슬롯머신을 광고 방면에서 활용할 때, 각 슬롯머신을 광고 배너, reward를 사용자의 클릭이라고 가정한다. 즉, 사용자의 클릭 확률이 높은 광고 배너를 찾는 것이 목표가 된다.
Thompson sampling은 각 배너의 클릭 확률을 베타 분포로 표현한다. 각 배너는 각기 다른 베타 분포를 보유하고 있다: 광고 A의 클릭률~BetaA+1, A+1, 광고 B의 클릭률~BetaB+1, B+1
여기서 𝛼는 광고 배너를 보고 클릭한 횟수, 𝛽는 광고 배너를 보고도 클릭하지 않은 횟수다. 다시 말해서, 𝛼를 고정한 채로 𝛽를 증가시키면 분포가 왼쪽으로 치우치며, 𝛽를 고정한 채로 𝛼를 증가시키면 분포가 오른쪽으로 치우치게 된다.
각 베타 분포의 그래프의 x축은 광고 배너의 클릭 확률, y축은 광고 배너의 클릭 확률이 x일 확률이다.
방식:
데이터가 어느 정도 쌓일 때까지 랜덤하게 광고를 노출.
데이터가 일정량 쌓이면 각 분포당 하나의 값을 샘플링. (과거의 노출 기록만 가지고 greedy하게 선별할 경우 최적의 값에 다다르지 못할 수 있기 때문에 샘플링 진행) 샘플값이 더 높은 분포에 해당하는 배너를 노출 후 변한 𝛼 또는 𝛽 값을 다시 베타분포에 반영.
분포 업데이트를 계속하면 CTR에 변화폭이 거의 0에 다다르게 됨. 해당 시점에서 탐색을 종료하고 활용에 들어갈 수 있음.
장점: 베타 분포를 사용함으로써 parameter 값에 상관없이 확률값이 [0, 1] 이내로 주어지고, 따라서 분포의 비교에도 매우 용이하다. 늦게 들어오는 피드백도 수용 가능하다.


SQL으로 조회 기반 Best, 구매 기반 Best, 카테고리별 Best를 구하는 쿼리를 작성해주세요.
SELECT
RANK() OVER (ORDER BY click DESC) AS rk1,
RANK() OVER (ORDER BY purchase DESC) AS rk2,
RANK() OVER (ORDER BY category DESC) AS rk3
FROM ___
LIMIT n


CTR이 더 높은데 구매전환율이 낮다면?
CTR를 손해 보지 않는 선에서 매출 타협을 하지 않는 것이 좋을 것으로 보인다. Business 파트 인사들과의 의논이 필수적이다.


두 추천엔진간의 성능 비교는 어떤 지표와 방법으로 할 수 있을까요? 검색엔진에서 쓰던 방법을 그대로 쓰면 될까요? 안될까요?
검색엔진에서 쓰던 방법? Precision과 Recall을 더 많이 보고, 짧은 duration rate과 높은 retention rate에도 신경을 쓸 것. 후자 두 가지는 대부분의 회사에서 그렇게 직관적인 지표로 해석하지는 않는다.
추천 시스템에서도 사용자 만족도 조사를 위해 duration rate와 retention rate를 고려할 수 있음. (단순 CTR보다 낫기 때문) 다만 검색엔진이 쓰기 힘든 지표들 (MAP@K, Intra-list Similarity 등)을 활용할 수 있기 때문에 더 나은 모델링이 가능할 것으로 보임.

어떤 피처 / 모델링을 해보고 싶은지?
카카오에서 추천 받았는데 개선하고 싶었던 추천
카톡 선물: 후기 랭킹 대신 구매 랭킹
카톡 선물: 커스텀 가능한 선물이나 미디어에 나온 제품 탭
멜론 티켓: 항목에 많은 체크를 하지 않은 cold start 유저에게 추천되는 첫 공연이 과도하게 많다. 따로 유사 공연을 n개 보여주고 관심도를 체크하거나 유사도 높은 소수의 공연만 추천 탭에 띄워주는 건?
카카오에서 추천 받았는데 좋았던 추천
멜론: ~~랑 비슷한 느낌 (그런 탭이 따로 있어도 좋을 듯, 가수를 입력하면 비슷한 장르를 추천해주기)

분석 일반
좋은 feature란 무엇인가요. 이 feature의 성능을 판단하기 위한 방법에는 어떤 것이 있나요?
Dataset에 자주 등장하며 분명하고 명확한 의미가 부여되어야 한다.


피처의 중요도 측정 방법으로는 무엇이 있나요?
Feature importance: 트리 기반 모델에서 사용. 각 feature가 information gain에 기반한 트리 분할에 얼마나 기여했는지 수치화. Overfitting될 위험이 있고 지표가 절대적이지 않음.
Permutation importance: feature마다 shuffle하며 성능 변화를 관찰하고 성능이 크게 떨어질 경우 해당 feature의 중요도가 높다고 판단. 정확도가 높으나 느림. 지표가 절대적이지 않음 (A/B Test로 보완)


고객이 원하는 예측모형을 두가지 종류로 만들었다. 하나는 예측력이 뛰어나지만 왜 그렇게 예측했는지를 설명하기 어려운 random forest 모형이고, 또다른 하나는 예측력은 다소 떨어지나 명확하게 왜 그런지를 설명할 수 있는 sequential bayesian 모형입니다. 고객에게 어떤 모형을 추천하겠습니까?
예측력이 필요한 상황 (암 여부 등)에서는 전자, 이유를 설명해야 할 상황 (대출 불가 이유)에서는 후자.


고객이 내일 어떤 상품을 구매할지 예측하는 모형을 만들어야 한다면 어떤 기법(예: SVM, Random Forest, logistic regression 등)을 사용할 것인지 정하고 이를 통계와 기계학습 지식이 전무한 실무자에게 설명해봅시다.
Decision Tree. 중요한 피처들(성별, 최근 구매 상품, 평균 구매 금액, 평균 매장 재방문 일자 등)을 직접 선별해 상품 구매 예측을 하고, 이를 기반으로 상품 군을 구매할 타겟을 나눔.

데이터 간의 유사도를 계산할 때, feature의 수가 많다면(예: 100개 이상), 이러한 high-dimensional clustering을 어떻게 풀어야할까요?
PCA.


딥러닝 일반
딥러닝은 무엇인가요? 딥러닝과 머신러닝의 차이는?
딥러닝은 머신러닝의 일부로, 인공신경망을 사용하여 머신러닝 학습을 수행한다.
머신러닝은 데이터의 여러 특징 중 무엇을 추출할지 사람이 직접 분석하고 판단하나 (feature engineering), 딥러닝에서는 기계가 자동으로 특징을 추출한다. 따라서 보다 큰 데이터셋과 긴 학습시간을 필요로 한다.
주로 정형 데이터는 머신러닝, 비정형 데이터는 딥러닝을 사용한다.


Cost Function과 Activation Function은 무엇인가요?
Cost function(비용 함수): 예측 값과 실제 값의 차이에 대한 함수 (즉, 최소화해야 한다)
Activation function (활성화 함수): 보다 복잡한 데이터를 예측할 수 있도록 선형 모델을 비선형 모델로 변환해주는 함수. 선형 모델의 함수와 활성화 함수가 결합되어 비선형 모델이 된다. 여기서 가중치(weight)와 편향(bias)이 중요하게 작용하는데, 가중치는 활성화 함수의 기울기를 조절하며, 편향은 함수를 수평이동하여 데이터에 더 잘 맞도록 한다. 연산값이 너무 많아지거나 불필요한 피처가 많을 때 해결하기 위한 함수이기도 하다.


알고있는 Activation Function에 대해 알려주세요. (Sigmoid, ReLU, LeakyReLU, Tanh 등)
Sigmoid: 입력을 0과 1 사이의 값으로 변환. 입력 값의 절대값이 높을수록 기울기를 소실하는 gradient vanishing 문제가 있음.
Tanh: 입력을 -1과 1 사이의 값으로 변환. 마찬가지로 gradient vanishing 문제.
ReLU: max(0, x). Gradient vanishing 문제가 없고 효율과 성능이 뛰어나나, 음수일 경우 Dead ReLU.
Leaky ReLU: max(0.01x, x). 좋은 성능을 유지하며 Dead ReLU 문제 해결.
Softmax: 모든 입력값에 대해 0과 1 사이의 수로 변환해 확률로 해석. (극소나 음수의 입력값은 0에 가깝도록 변환) 다중 클래스 변환에서 많이 사용.


딥러닝에서 overfitting일 경우 어떻게 대처해야 할까요?
Early stopping: validation loss가 높아지는 시점에서 training epoch 추가를 중단.
정규화: L2나 L1 정규화로 비용함수의 weight의 크기에 페널티 부과.
Data augmentation: (특히 이미지 데이터일 때) 데이터의 수가 적을 경우, 인위적으로 mirroring 등의 변화를 주어 훈련 데이터의 수를 증가.
Noise robustness: 노이즈나 이상치의 면역을 늘릴 수 있도록 데이터나 weight에 의도적인 노이즈 부여.
Batch-normalization: 활성화 함수의 활성화값 (출력값)을 정규화. 각 은닉층에서 정규화를 하며 input 분포가 일정해지고, learning rate의 영향을 덜 받게 된다. 결과적으로 학습속도가 빨라진다.


Batch Normalization의 효과는?
Mini-batch 단위로 입력 데이터의 평균을 0, 분산을 1로 표준화시키는 것. 네트워크를 거친 값들의 분포를 가우시안으로 변환. 기울기 소실 문제에서 자유로워 learning rate를 크게 설정할 수 있고, 따라서 학습 속도도 빨라진다. 또한 초기 가중치 값에 크게 의지하지 않아도 된다 (어차피 표준화되기 때문)


Hyperparameter는 무엇인가요?
사용자가 직접 세팅해주는 값. 튜닝 기법으로는 Grid Search, Manual Search, Random Search 등.


요즘 Sigmoid 보다 ReLU를 많이 쓰는데 그 이유는?
Vanishing gradient 문제를 해결하기 위해서. 다만 ReLU도 Dead ReLU 문제가 있기 때문에 Leaky ReLU로 이를 보완할 수 있다.


Gradient Descent에 대해서 쉽게 설명한다면?
특정 함수의 최소값(혹은 최적값)을 찾기 위해 미분하여 기울기를 구하고 경사 하강 방향으로 이동해 parameter 값을 도출.
단점: learning rate에 따라 결과값이 다르거나 결과 도출이 불가능할 수 있다. Local minima 문제에서 자유롭지 못하다.


hyperparameter 튜닝 기법의 차이점은?
Grid search: hyperparameter의 모든 조합들이 시도되고 검증됨. 정확하나, 당연히 hyperparameter 개수가 많을수록 튜닝에 차질. 10-fold CV까지 행할 경우 iteration 더욱 증가.
Random search: 주어진 iteration 수에 따라 random하게 조합을 설정해 검증. Grid search보다는 정확성이 떨어지지만 꽤 정확한 결과를 도출하며 훨씬 더 빠른 시간 내에 연산 가능.
Bayesian optimization: 이전 hyperparameter 조합에 근거해 다음 조합 설정. 각 조합 연산 설정에는 시간이 조금 더 들지만, iteration 수가 급감하며 더욱 빠르게 연산 가능.


Mini-batch를 작게 할 때의 장단점은?
장점: 적은 계산 비용, 빠른 속도, 기울기가 비교적 random해 local minima 문제에서 보다 자유로움
단점: 큰 mini-batch 사이즈보다 부정확한 기울기


마지막으로 읽은 논문은 무엇인가요? 설명해주세요.

통계
CLT(중심극한정리)는 무엇이며, 왜 유용한 걸까요?
데이터의 양이 많을수록 데이터의 분포는 정규분포와 가까워진다. 따라서 수학적 확률 판단이 용이.


하지만 또 요즘같은 빅데이터 시대에는 정규성 테스트가 의미 없다는 주장이 있습니다. 맞을까요?
데이터의 양이 많아지기에 CTL에 따라 데이터가 정규분포화 되기에, 정규성 테스트가 의미 없다고 주장하는 사람들도 있다. 다만 context를 충분히 확인해야 할 문제.


Type 1 error와 Type 2 error는 어떻게 다른가요? 검정력(Statistical power)과 무슨 관계가 있나요?
Type 1 error (): H0가 사실이지만 거짓으로 판정해 reject할 확률.
Type 2 error (): H0가 거짓이지만 사실로 판정해 reject에 실패할 확률.
검정력: H0가 거짓일 때 거짓으로 판정할 확률. 1 - 로 계산한다.


고유값(eigenvalue)와 고유벡터(eigenvector)에 대해 설명해주세요. 왜 중요할까요?
정방행렬 A와 벡터 v의 곱이 값 와 v의 곱이 되었을 때 (선형 변환), 와 v가 각각 고유값과 고유벡터다.
SVD, PCA 등 행렬 분해 및 차원 축소 기법에 유용하다.


결측치가 있을 경우 채워야 할까요? 그 이유는 무엇인가요? 결측치에 평균값(mean)을 채워 넣는 건 어떨까요?
결측치를 처리하는 방법에는 그대로 놔두기, 제거하기, 특정 값으로 채우기, 예측 값으로 채우기가 있다.
그대로 놔두기: 결측값을 잘 고려하는 모델 (xgboost)은 괜찮으나, 고려하지 않는 모델 (선형 회귀)의 경우 결측치 때문에 학습이 어렵거나 불가능하다.
데이터 제거: 만약 피처가 많을 경우, 단 하나의 결측치 때문에 모든 특성을 제거해야 하는 일이 벌어진다. (전체 데이터의 수가 많고 피처 수가 적다면 괜찮을 방법일 수도.)
특정 값 채우기: 결측치가 많을수록 무의미. Context에 따라 0 등의 값으로 채우는 건 괜찮은 방법일 수 있다.
예측 값 채우기 (평균, 중앙값, k-NN 등): 이 역시 결측치가 많을수록 무의미. Context에 따라 다름. 분포를 고려해야 하는 추가적인 확인이 필요하다.


이상치(outlier)를 판단하는 기준은 무엇인가요?
IQR (Q3 - Q1) 기법: Q1 - 1.5*IQR 보다 낮거나, Q3 + 1.5*IQR 보다 높다면 이상치.
Z-score 사용: 가우시안 분포일 때만 사용 가능. 특정 z-score 값보다 높거나 낮다면 이상치.


이상치는 어떻게 처리해야 하나요?
Context에서 벗어난 값일 경우 삭제.
극도로 높거나 낮은 값일 경우, context에 맞지 않다면 삭제.
삭제가 불가능한 이상치일 경우:
다른 모델 사용 (선형 모델 대신 비선형 모델로?)
데이터 정규화 (normalization)로 스케일링
Random forest 등 이상치에 영향을 덜 받는 모델 사용


대수의 법칙(Law of Large Numbers)이란 무엇인가요?
데이터가 많아질수록 sample mean과 sample variance는 population mean과 population variance에 가까워진다.


Confounding variable이란 무엇인가요?
독립변수가 아니면서 종속변수에 큰 영향을 미치는 변수.


p-value란?
Null hypothesis(H0)이 참이라 가정했을 때, 표본의 통계치와 같거나 더 극단적인 통계치가 관측될 확률. 주로 significance level (ɑ)을 정하곤 하는데, p-value가 ɑ보다 높다면 H0 유지, 낮다면 H0을 reject한다.


p-value는 요즘 시대에도 여전히 유효할까요? 언제 p-value가 실제를 호도하는 경향이 있을까요?
p-value는 데이터의 양이 많아지면 낮아질 수 있다. 표본이 커지면 표본 오차가 작아지기 때문이다. 따라서 빅데이터 시대에는 p-value의 기준값이 5%보다 더 낮아야 할 수도 있다.


Bias와 variance는 어떻게 조율할 수 있을까요?
Bias: 데이터의 representativeness 확인, 앙상블 모델 (부스팅)
Variance: 학습 데이터 추가, 정규화 (Regularization), 앙상블 모델 (배깅)


어떨 때 모수적 방법론을 쓸 수 있고, 어떨 때 비모수적 방법론을 쓸 수 있나요?
비모수적 (nonparametric): 표본 수가 30개 이하이거나, 정규 분포를 따르지 않을 때
모수적 (parametric): 데이터가 정규 분포를 따를 때 (또는 30개 이상의 데이터가 있을 때)


“likelihood”와 “probability”의 차이는 무엇일까요?
확률(probability): 논리적 접근. 어떤 시행에서 특정 결과가 나올 가능성으로, 총합은 1.
가능도(likelihood): 실재적 접근. 어떤 시행을 충분히 실행한 이후 그 결과를 토대로 경우의 수의 가능성 도출. 합의 제한은 없음.


Prior probability, likelihood, marginal likelihood를 나이브 베이즈와 관련지어 설명하세요.
베이즈 정리는 조건부 확률을 표현한다. P(A|B)=P(A)P(B|A)P(B) 
이를 용어로 변환하여 표현하면 다음과 같다. Posterior=PriorLikelihoodEvidence


베이지안과 프리퀀티스트간의 입장차이를 설명해주실 수 있나요?
베이지안: 과거의 사건이 현재 사건에 영향을 끼친다.
프리퀀티스트: 현재의 객관적 사건에 의해서만 사건이 발생한다.


베르누이 분포 / 이항 분포 / 카테고리 분포 / 다항 분포 / 가우시안 정규 분포 / t 분포 / 카이제곱 분포 / F 분포 / 베타 분포 / 감마 분포 / 디리클레 분포에 대해 설명해주세요. 혹시 연관된 분포가 있다면 연관 관계를 설명해주세요.


신뢰 구간의 정의는 무엇인가요? 어떻게 계산하나요? 점추정과의 차이는 무엇인가요?
샘플의 parameter가 위치해 있을 것으로 신뢰하는 구간.
95% 신뢰 구간이 주어질 경우, parameter가 95%의 확률로 해당 구간에 위치한다는 것이 아니라, 같은 방식으로 샘플링을 100번 했을 때 95개의 샘플에서 parameter가 해당 구간 내에 위치한다는 뜻.


로그 함수는 어떤 경우 유용합니까? 사례를 들어 설명해주세요.
단위 수가 너무 클 경우, 회귀를 바로 진행하면 결과를 왜곡할 수 있기 때문에 로그 적용.
또한 비선형 관계의 데이터를 선형화할 때 사용.
자연어 처리
One-hot encoding에 대해 설명해주세요.
질적 피처를 value의 수만큼 0-1 binary 피처로 변환하는 feature engineering.


뉴스 기사를 주제별로 자동 분류하는 시스템을 어떻게 구축할까요?
텍스트를 모델에 입력할 수 있는 embedding 방식을 사용하고, NLP 모델 적용.


Stop Words는 무엇일까요? 이것을 왜 제거해야 하나요?
의미(와 중요도)가 떨어지지만 빈도수가 많아 NLP 모델을 해치는, ‘the’ 등의 단어들.


영화 리뷰가 긍정적인지 부정적인지 예측하기 위해 모델을 어떻게 설계하시겠나요?
이진분류 모델 (https://codingcrews.github.io/2019/01/11/imdb/)


TF-IDF 점수는 무엇이며 어떤 경우 유용한가요?
단어의 빈도수를 활용해 만든 중요도 지표. 단어의 중요도 및 단어 간의 유사도를 계산할 수 있다. 다만 문맥을 파악하는 것이 아니라 빈도 수로 결정하는 것이기 때문에 비슷한 뜻도 다르게 표현되면 낮은 유사도를 보인다.


PageRank 알고리즘은 어떻게 작동하나요?
Degree centrality: 노드의 개수로 중심도 판별. 인용 노드의 중심도를 고려하지 않는다는 단점이 존재.
Eigenvector centrality: 인용 노드의 중심도를 고려해 중심도 판별. In-degree가 없는 노드들은 활발한 활동에도 중요도가 0이라는 단점이 존재.
Katz centrality: 와  parameter를 사용해 중요도 0 노드 문제 해결; 는 in-degree와 관계없이 각 노드에 부여되는 중심도, 는 인용으로부터 오는 중심도와 의 균형을 조정하는 변수. 인용하는 모든 노드에 동일 중요도를 부여한다는 단점이 존재.
PageRank centrality: 중요도가 높은 노드라도 얼마나 많은 노드를 인용하느냐에 따라 중요도가 다르게 부여된다는 것을 반영. 즉, 중요도가 높다고 하더라도 수천 개의 인용이 있다면 부여하는 중요도가 낮아진다. 알고리즘 공식은 xi=jAijxjkjout+.


Word2Vec의 원리는?
예측 기반으로 단어를 벡터화하는 비지도학습 Neural Network 모델 (은닉층 2개). 저차원벡터를 다차원 공간에 벡터화해서 유사성을 표현한다. 카테고리형 변수의 one-hot encoding된 벡터를 은닉층으로 보내고, softmax로 변환 후 output 벡터로 변환.
데이터의 양이 많거나 복잡 데이터라면, 차원을 정해놓고 embedding.
CBOW: 주변 n개 단어들로 중심 단어 유추.
Skip-gram: 중심 단어로 주변 단어들 유추.

https://www.ubuntupit.com/frequently-asked-machine-learning-interview-questions-and-answers/
https://rpubs.com/JDAHAN/172473?lipi=urn%3Ali%3Apage%3Ad_flagship3_pulse_read%3BgFdjeopHQ5C1%2BT367egIug%3D%3D
https://www.analyticsvidhya.com/blog/2017/01/must-know-questions-deep-learning/
https://www.cpuheater.com/deep-learning/deep-learning-interview-questions-and-answers/


https://github.com/boostcamp-ai-tech-4/ai-tech-interview/tree/main/answers
https://yongwookha.github.io/MachineLearning/2021-01-29-interview-question
https://github.com/hongym7/dictionary
https://jayhong1999.notion.site/06c5c33f7ccf47bfa4f428bed1b0e893
https://shelling203.tistory.com/35

https://stellarway.tistory.com/8




추가질문: 머신러닝

회귀 모델에서 회귀 계수를 꼭 필요로 하나요?
Step-wise regression은 무엇인가요? Step-wise 기법에 대해 알고 있나요? (27)
Image processing과 머신러닝은 어떻게 연관되어 있나요?
머신러닝 알고리즘을 parallelize하는 tool로는 무엇이 있을까요? Parallelize란?
푸리에 변환에 대해 설명하세요.
데이터 마이닝과 머신러닝의 차이는 무엇인가요?
AI와 머신러닝의 차이는 무엇인가요?
Convex function과 Non-convex function의 차이는 무엇인가요?
최적화 기법중 Newton’s Method 방법에 대해 알고 있나요?
Maximal margin classifier란 무엇인가요? 해당 margin을 어떻게 처리할 수 있나요? (37)
새로운 credit risk scoring 모델이 잘 작동한다는 것을 어떻게 알 수 있을까요? (18)
N-gram에 대해 설명해 보세요. (19)
Artificial Neural Network는 무엇이며, back propagation은 무엇인가요? (41)
추가질문: 딥러닝
Back Propagation에 대해서 쉽게 설명 한다면?
Local Minima 문제에도 불구하고 딥러닝이 잘 되는 이유는? GD가 Local Minima 문제를 피하는 방법은? 찾은 해가 Global Minimum인지 아닌지 알 수 있는 방법은?
RMSprop, Adam에 대해서 아는대로 설명한다면?
딥러닝할 때 GPU를 쓰면 좋은 이유는?
TF, Keras, PyTorch 등을 사용할 때 디버깅 노하우는?
뉴럴넷의 가장 큰 단점은 무엇인가? 이를 위해 나온 One-Shot Learning은 무엇인가?
추가질문: 통계
확률 모형과 확률 변수는 무엇일까요?
누적 분포 함수와 확률 밀도 함수는 무엇일까요? 수식과 함께 표현해주세요
데이터의 selection bias는 무엇인가요? 왜 중요한가요? 왜 결측치를 채워 넣는 것이 selection bias를 더 악화시킬 수도 있을까요?
Lift, KPI, robustness, model fitting, 실험 디자인, 80/20 규칙에 대해 설명해주세요.
필요한 표본의 크기를 어떻게 계산합니까?
ANOVA는 언제 사용하나요?
Factor analysis와 PCA의 차이는 무엇인가요?
Long-tailed 분포란 무엇인가요? 예를 들어 설명해 주세요. 왜 회귀와 분류 문제에 중요할까요?
실험 데이터가 행동에 미치는 영향을 예를 들어 설명해 주세요.
실험 데이터는 관찰 데이터와 어떻게 다른가요?
Quality assurance와 six sigma에 대해 설명해주세요.
가우시안 분포나 log-normal 분포를 띠지 않는 예를 들어보세요.
Root cause 분석은 무엇인가요? "상관관계는 인과관계를 의미하지 않는다"라는 말이 있습니다. 설명해주실 수 있나요?
추가질문: NLP
문장에서 “Apple”이란 단어가 과일인지 회사인지 식별하는 모델을 어떻게 훈련시킬 수 있을까요?
뉴스 기사에 인용된 텍스트의 모든 항목을 어떻게 찾을까요?
음성 인식 시스템에서 생성된 텍스트를 자동으로 수정하는 시스템을 어떻게 구축할까요?
잠재론적, 의미론적 색인은 무엇이고 어떻게 적용할 수 있을까요?
POS 태깅은 무엇인가요? 가장 간단하게 POS tagger를 만드는 방법은 무엇일까요?
dependency parsing란 무엇인가요?
Translate 과정 Flow에 대해 설명해주세요
한국어에서 많이 사용되는 사전은 무엇인가요?
Regular grammar는 무엇인가요? regular expression과 무슨 차이가 있나요?
RNN에 대해 설명해주세요
LSTM은 왜 유용한가요?
영어 텍스트를 다른 언어로 번역할 시스템을 어떻게 구축해야 할까요?
추가질문: 서비스 구현
신규 방문자와 재 방문자를 구별하여 A/B 테스트를 하고 싶다면 어떻게 해야 할까요?
쇼핑몰의 상품별 노출 횟수와 클릭수를 손쉽게 수집하려면 어떻게 해야 할까요?
당장 10분안에 사이트의 A/B 테스트를 하고 싶다면 어떻게 해야 할까요? 타 서비스를 써도 됩니다.
R의 결과물을 python으로 만든 대시보드에 넣고 싶다면 어떤 방법들이 가능할까요?
여러 웹사이트를 돌아다니는 사용자를 하나로 엮어서 보고자 합니다. 우리가 각 사이트의 웹에 우리 코드를 삽입할 수 있다고 가정할 때, 이것이 가능한가요? 가능하다면, 그 방법에는 어떤 것이 있을까요?

추가질문: 시각화
"신규/재방문자별 지역별(혹은 일별) 방문자수와 구매전환율"이나 "고객등급별 최근방문일별 고객수와 평균구매금액"와 같이 4가지 이상의 정보를 시각화하는 가장 좋은 방법을 추천해주세요
구매에 영향을 주는 요소의 발견을 위한 관점에서, 개인에 대한 쇼핑몰 웹 활동의 시계열 데이터를 효과적으로 시각화하기 위한 방법은 무엇일까요? 표현되어야 하는 정보(feature)는 어떤 것일까요? 실제시 어떤 것이 가장 고민될까요?
파이차트는 왜 구릴까요? 언제 구린가요? 안구릴때는 언제인가요?
히스토그램의 가장 큰 문제는 무엇인가요?
워드클라우드는 보기엔 예쁘지만 약점이 있습니다. 어떤 약점일까요?
어떤 1차원값이, 데이터가 몰려있어서 직선상에 표현했을 때 보기가 쉽지 않습니다. 어떻게 해야할까요?

추가질문: 비즈니스/커뮤니케이션
고객이 궁금하다고 말하는 요소가 내가 생각하기에는 중요하지 않고 다른 부분이 더 중요해 보입니다. 어떤 식으로 대화를 풀어나가야 할까요?
현업 카운터 파트와 자주 만나며 실패한 분석까지 같이 공유하는 경우와, 시간을 두고 멋진 결과만 공유하는 케이스에서 무엇을 선택하시겠습니까?
고객이 질문지 리스트를 10개를 주었습니다. 어떤 기준으로 우선순위를 정해야 할까요?
오프라인 데이터가 결합이 되어야 해서, 데이터의 피드백 주기가 매우 느리고 정합성도 의심되는 상황입니다. 우리가 할 수 있는 액션이나 방향 수정은 무엇일까요?
동시에 여러개의 A/B테스트를 돌리기엔 모수가 부족한 상황입니다. 어떻게 해야할까요?
고객사들은 기존 추천서비스에 대한 의문이 있습니다. 주로 매출이 실제 오르는가 하는 것인데, 이를 검증하기 위한 방법에는 어떤 것이 있을까요? 위 관점에서 우리 서비스의 성능을 고객에게 명확하게 인지시키기 위한 방법을 생각해봅시다.
